{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14c16169-249c-48fb-b2db-abde63ee0f3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:33:21.614419Z",
     "start_time": "2025-07-16T06:33:21.609160Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from minsearch import AppendableIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44ccc1a0-d2be-4783-810f-215dd9c7a866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:34:05.636022Z",
     "start_time": "2025-07-16T06:34:05.107448Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "483e5565-8d81-4897-bc3a-50dd1c5b871d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:34:07.517094Z",
     "start_time": "2025-07-16T06:34:07.113184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x736b2f199ac0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb005100-6417-4e6e-9d36-eb3b55416522",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:34:10.712507Z",
     "start_time": "2025-07-16T06:34:10.709564Z"
    }
   },
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f5e6af693f61e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n"
     ]
    }
   ],
   "source": [
    "results = search('I just discovered the course. Can I join now?')\n",
    "print(results[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1935842125eaf368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e3eaf8502de1116",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d392d86276677af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "817a30ea-5855-4143-9b1a-258b72e16550",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T13:07:06.017398Z",
     "start_time": "2025-07-15T13:07:03.081001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, you can still join the course now. Even if you don't register, you're eligible to submit homework assignments. However, keep in mind that there will be deadlines for final projects, so it’s best not to leave everything until the last minute.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('I just discovered the course. Can I join now?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c521c75-ccfd-46ee-938b-9dc742ead384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T13:07:10.593755Z",
     "start_time": "2025-07-15T13:07:08.523915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The provided context does not include specific information or instructions on how to run Docker on Gentoo. Therefore, I cannot provide an answer based on the existing context. Please refer to Gentoo's official documentation or community resources for guidance on running Docker on that specific operating system.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('how do I run docker on gentoo?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6147a0f575953271",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "At the beginning the context is EMPTY.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "If CONTEXT is EMPTY, you can use our FAQ database.\n",
    "In this case, use the following output template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\"\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b37ffc161217da21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "At the beginning the context is EMPTY.\n",
      "\n",
      "<QUESTION>\n",
      "how do I run docker on gentoo?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT> \n",
      "EMPTY\n",
      "</CONTEXT>\n",
      "\n",
      "If CONTEXT is EMPTY, you can use our FAQ database.\n",
      "In this case, use the following output template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\"\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I run docker on gentoo?\"\n",
    "context = \"EMPTY\"\n",
    "\n",
    "prompt = prompt_template.format(question=question, context=context)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4149b195-908a-4b6e-bdd8-658be48b8d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"To run Docker on Gentoo, you can follow these steps:\\n\\n1. **Install Docker**: You can install Docker on Gentoo using the package manager. Open your terminal and run:\\n   ```\n",
      "   emerge app-emulation/docker\\n   ```\\n   This command will download and install Docker along with its dependencies.\\n\\n2. **Add User to Docker Group**: After the installation, add your user to the 'docker' group to manage Docker without needing root permissions:\\n   ```\n",
      "   usermod -aG docker your_username\\n   ```\\n   Remember to replace 'your_username' with your actual username.\\n\\n3. **Start Docker Daemon**: You need to start the Docker service. You can do that with systemd or OpenRC, depending on your setup. For OpenRC, run:\\n   ```\n",
      "   rc-service docker start\\n   ```\\n   For systemd, use:\\n   ```\n",
      "   systemctl start docker\\n   ```\\n\\n4. **Enable Docker to Start at Boot**: You may want Docker to start automatically when the system boots. For OpenRC, run:\\n   ```\n",
      "   rc-update add docker default\\n   ```\\n   For systemd, run:\\n   ```\n",
      "   systemctl enable docker\\n   ```\\n\\n5. **Verify Installation**: To ensure Docker is installed and running correctly, you can run:\\n   ```\n",
      "   docker --version\\n   ```\\n   And test Docker by running:\\n   ```\n",
      "   docker run hello-world\\n   ```\\n   This will download a test image and run it, indicating that Docker is functioning as expected.\\n\\nMake sure you have the necessary kernel support for Docker, like cgroups and overlay filesystems, enabled in your Gentoo kernel configuration.\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "answer = llm(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db6574e6-aded-43bd-9836-2e3d834c4341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"The student asked how to join the course, but the context does not provide information on enrollment procedures. Therefore, I need to refer to the FAQ database for accurate instructions.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I join the course?\"\n",
    "context = \"EMPTY\"\n",
    "\n",
    "prompt = prompt_template.format(question=question, context=context)\n",
    "answer = llm(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5694d89b-eef9-4fe2-b7fe-93cf0fa4b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "\n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de0d0077-1a49-4670-ad2d-790b76a826ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "At the beginning the context is EMPTY.\n",
      "\n",
      "<QUESTION>\n",
      "how do I join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - Can I still join the course after the start date?\n",
      "answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "</CONTEXT>\n",
      "\n",
      "If CONTEXT is EMPTY, you can use our FAQ database.\n",
      "In this case, use the following output template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\"\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "search_results = search(question)\n",
    "context = build_context(search_results)\n",
    "prompt = prompt_template.format(question=question, context=context)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1a9d8e6-ba16-405d-b879-028680a545aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"To join the course, you need to register before the start date using the provided registration link. Additionally, you should subscribe to the course public Google Calendar to stay updated and join the course's Telegram channel for announcements. Remember, even if you miss the registration deadline, you are still eligible to submit your homework assignments.\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "answer = llm(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30fa9499-9e9e-4630-8926-1d515b6473bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_rag_v1(question):\n",
    "    context = \"EMPTY\"\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(answer)\n",
    "\n",
    "    if answer['action'] == 'SEARCH':\n",
    "        print('need to perform search...')\n",
    "        search_results = search(question)\n",
    "        context = build_context(search_results)\n",
    "        \n",
    "        prompt = prompt_template.format(question=question, context=context)\n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(answer)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0181a69-51db-4c82-9bd7-263a075bb7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'ANSWER', 'answer': \"To join the course, you typically need to visit the course's official website or the platform it's being offered on. Look for a 'Sign Up' or 'Enroll' button, fill out the registration form, and follow the instructions provided to complete your enrollment. If there is an application process or prerequisites, be sure to review those beforehand.\", 'source': 'OWN_KNOWLEDGE'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To join the course, you typically need to visit the course's official website or the platform it's being offered on. Look for a 'Sign Up' or 'Enroll' button, fill out the registration form, and follow the instructions provided to complete your enrollment. If there is an application process or prerequisites, be sure to review those beforehand.\",\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_rag_v1('how do I join the course?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f0e02cf-1088-46c2-b788-d27940d1617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'ANSWER', 'answer': \"To patch KDE under FreeBSD, you can follow these general steps: 1. First, ensure you have the necessary development tools installed. You can do this by installing the `ports` collection if you haven't already. You can enable ports in the `/etc/make.conf` file by adding `WITH_PKGNG=yes`. 2. Navigate to the KDE ports directory that corresponds to the version of KDE you want to patch. You can find KDE ports in `/usr/ports/x11/kde5`, for example. 3. Apply your patch files to the source code. You can use the `patch` command, which takes the patch file and the target file as arguments. For example: `patch < my_patch.patch`. 4. After applying the patch, navigate to the port directory and update the port with `make install clean` to rebuild and install the patched version. 5. Finally, restart your KDE session to apply the changes. Always ensure you have backups and test patches in a development environment when possible.\", 'source': 'OWN_KNOWLEDGE'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To patch KDE under FreeBSD, you can follow these general steps: 1. First, ensure you have the necessary development tools installed. You can do this by installing the `ports` collection if you haven't already. You can enable ports in the `/etc/make.conf` file by adding `WITH_PKGNG=yes`. 2. Navigate to the KDE ports directory that corresponds to the version of KDE you want to patch. You can find KDE ports in `/usr/ports/x11/kde5`, for example. 3. Apply your patch files to the source code. You can use the `patch` command, which takes the patch file and the target file as arguments. For example: `patch < my_patch.patch`. 4. After applying the patch, navigate to the port directory and update the port with `make install clean` to rebuild and install the patched version. 5. Finally, restart your KDE session to apply the changes. Always ensure you have backups and test patches in a development environment when possible.\",\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_rag_v1('how patch KDE under FreeBSD?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be6bcf-e7ff-492c-8f62-3025742bce5f",
   "metadata": {},
   "source": [
    "## Agentic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87973665-b4f3-4a96-b3a5-8bb0edb57a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "\n",
    "The CONTEXT is build with the documents from our FAQ database.\n",
    "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
    "from FAQ to and add them to the context.\n",
    "PREVIOUS_ACTIONS contains the actions you already performed.\n",
    "\n",
    "At the beginning the CONTEXT is empty.\n",
    "\n",
    "You can perform the following actions:\n",
    "\n",
    "- Search in the FAQ database to get more data for the CONTEXT\n",
    "- Answer the question using the CONTEXT\n",
    "- Answer the question using your own knowledge\n",
    "\n",
    "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
    "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
    "\n",
    "Don't use search queries used at the previous iterations.\n",
    "\n",
    "\n",
    "Don't repeat previously performed actions.\n",
    "\n",
    "Don't perform more than {max_iterations} iterations for a given student question.\n",
    "The current iteration number: {iteration_number}. If we exceed the allowed number \n",
    "of iterations, give the best possible answer with the provided information.\n",
    "\n",
    "\n",
    "Output templates:\n",
    "\n",
    "If you want to perform search, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\",\n",
    "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER_CONTEXT\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<SEARCH_QUERIES>\n",
    "{search_queries}\n",
    "</SEARCH_QUERIES>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "<PREVIOUS_ACTIONS>\n",
    "{previous_actions}\n",
    "</PREVIOUS_ACTIONS>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e7f39f7-aeb6-42fa-b010-0b366f19967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "how do I join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I join the course?\"\n",
    "\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []\n",
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=3,\n",
    "    iteration_number=1\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "29f5989b-949f-478a-abea-048b1bcfa1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)\n",
    "answer = json.loads(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95372972-6767-4c98-ba68-57a45326918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_actions.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b6ee2191-3035-427c-83f8-a6516ec6ea53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'SEARCH',\n",
       "  'reasoning': 'To provide specific instructions on how to join the course, I need to find relevant information from the FAQ database regarding enrollment procedures or joining processes.',\n",
       "  'keywords': ['how to enroll in the course',\n",
       "   'joining procedures for the course',\n",
       "   'course registration instructions']}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d4097b3d-2d46-4ba0-9444-da92d604cd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"To provide specific instructions on how to join the course, I need to find relevant information from the FAQ database regarding enrollment procedures or joining processes.\",\n",
      "  \"keywords\": [\n",
      "    \"how to enroll in the course\",\n",
      "    \"joining procedures for the course\",\n",
      "    \"course registration instructions\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(answer, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f9d091f8-8765-4012-971e-ad4498cab7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = answer['keywords']\n",
    "search_queries.extend(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "08d93d61-5762-435d-a388-19c71010b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in keywords:\n",
    "    res = search(k)\n",
    "    search_results.extend(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dd6e476c-fef7-4b57-9fdc-79e3a8587de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f35dfffd-82ec-492c-acf1-e7245a80578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(seq):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for el in seq:\n",
    "        _id = el['_id']\n",
    "        if _id in seen:\n",
    "            continue\n",
    "        seen.add(_id)\n",
    "        result.append(el)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ddb8a453-60a4-436d-a4db-10191cf3c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = dedup(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ae4336eb-7d37-4037-956e-2e21bea39f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d3eaa30b-edc7-449a-a6cd-b80f6081b862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "how do I join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "how to enroll in the course\n",
      "joining procedures for the course\n",
      "course registration instructions\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\n",
      "answer: You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide specific instructions on how to join the course, I need to find relevant information from the FAQ database regarding enrollment procedures or joining processes.\", \"keywords\": [\"how to enroll in the course\", \"joining procedures for the course\", \"course registration instructions\"]}\n",
      "</PREVIOUS_ACTIONS>\n"
     ]
    }
   ],
   "source": [
    "# question = \"how do I join the course?\"\n",
    "\n",
    "# search_queries = []\n",
    "# search_results = []\n",
    "# previous_actions = []\n",
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=3,\n",
    "    iteration_number=2\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ff78c687-a2c2-439b-a0af-19fbc09d94b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"action\": \"ANSWER_CONTEXT\",\n",
      "  \"answer\": \"To join the course, you need to register before it starts using the provided registration link and join the course's public Google Calendar as well as its Telegram channel for announcements. You don't need a confirmation email; you are accepted once you register, and you can start learning and submitting homework immediately without being on a registered list.\",\n",
      "  \"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "answer_json = llm(prompt)\n",
    "answer = json.loads(answer_json)\n",
    "print(json.dumps(answer, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "652db0b9-fbdd-4df5-b973-ac6cc0711213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"To provide a comprehensive answer, I need to find specific strategies or tips for success in Module 1. It would be helpful to look for resources on effective study habits, understanding the module's objectives, and other students' experiences.\",\n",
      "  \"keywords\": [\n",
      "    \"success in Module 1\",\n",
      "    \"study tips for Module 1\",\n",
      "    \"Module 1 best practices\",\n",
      "    \"effective learning strategies\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "success in Module 1\n",
      "effective learning strategies\n",
      "Module 1 best practices\n",
      "study tips for Module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e…\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\n",
      "answer: Error raised during the jupyter notebook’s cell execution:\n",
      "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\n",
      "Solution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).\n",
      "answer: More info in the Docker Docs on Best Practises\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\n",
      "answer: You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - how many Zoomcamps in a year?\n",
      "answer: There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\n",
      "Data-Engineering (Jan - Apr)\n",
      "MLOps (May - Aug)\n",
      "Machine Learning (Sep - Jan)\n",
      "There's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\n",
      "They follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Homework and Leaderboard - what is the system for points in the course management platform?\n",
      "answer: After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you’ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\n",
      "(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)\n",
      "\n",
      "section: General course-related questions\n",
      "question: Environment - Do I need both GitHub Codespaces and GCP?\n",
      "answer: Choose the approach that aligns the most with your idea for the end project\n",
      "One of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Environment - I want to use AWS. May I do that?\n",
      "answer: Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\n",
      "The problem would be when you need help. You’d need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\n",
      "Also see Is it possible to use x tool instead of the one tool you use?\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a comprehensive answer, I need to find specific strategies or tips for success in Module 1. It would be helpful to look for resources on effective study habits, understanding the module's objectives, and other students' experiences.\", \"keywords\": [\"success in Module 1\", \"study tips for Module 1\", \"Module 1 best practices\", \"effective learning strategies\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"The existing context does not provide specific strategies or tips for being successful in Module 1. I need to find additional resources or insights that detail effective study methods, time management tips, or common pitfalls to avoid for students focusing on Module 1: Docker and Terraform.\",\n",
      "  \"keywords\": [\n",
      "    \"Module 1 success strategies\",\n",
      "    \"effective study habits for Docker and Terraform\",\n",
      "    \"tips for succeeding in Module 1\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #2...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "tips for succeeding in Module 1\n",
      "effective study habits for Docker and Terraform\n",
      "Module 1 best practices\n",
      "success in Module 1\n",
      "Module 1 success strategies\n",
      "effective learning strategies\n",
      "study tips for Module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e…\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\n",
      "answer: Error raised during the jupyter notebook’s cell execution:\n",
      "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\n",
      "Solution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).\n",
      "answer: More info in the Docker Docs on Best Practises\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\n",
      "answer: You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - how many Zoomcamps in a year?\n",
      "answer: There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\n",
      "Data-Engineering (Jan - Apr)\n",
      "MLOps (May - Aug)\n",
      "Machine Learning (Sep - Jan)\n",
      "There's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\n",
      "They follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Homework and Leaderboard - what is the system for points in the course management platform?\n",
      "answer: After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you’ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\n",
      "(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)\n",
      "\n",
      "section: General course-related questions\n",
      "question: Environment - Do I need both GitHub Codespaces and GCP?\n",
      "answer: Choose the approach that aligns the most with your idea for the end project\n",
      "One of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Environment - I want to use AWS. May I do that?\n",
      "answer: Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\n",
      "The problem would be when you need help. You’d need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\n",
      "Also see Is it possible to use x tool instead of the one tool you use?\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Error creating Bucket: googleapi: Error 403: Permission denied to access ‘storage.buckets.create’\n",
      "answer: The error:\n",
      "Error: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\n",
      "The solution:\n",
      "You have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.g\n",
      "answer: You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Where can I find the Terraform 1.1.3 Linux (AMD 64)?\n",
      "answer: Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Install for WSL\n",
      "answer: https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Error:Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout\n",
      "answer: The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a comprehensive answer, I need to find specific strategies or tips for success in Module 1. It would be helpful to look for resources on effective study habits, understanding the module's objectives, and other students' experiences.\", \"keywords\": [\"success in Module 1\", \"study tips for Module 1\", \"Module 1 best practices\", \"effective learning strategies\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"The existing context does not provide specific strategies or tips for being successful in Module 1. I need to find additional resources or insights that detail effective study methods, time management tips, or common pitfalls to avoid for students focusing on Module 1: Docker and Terraform.\", \"keywords\": [\"Module 1 success strategies\", \"effective study habits for Docker and Terraform\", \"tips for succeeding in Module 1\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER\",\n",
      "  \"answer\": \"To be successful in Module 1: Docker and Terraform, here are some general tips that can help:\\n\\n1. **Understand the Basics**: Familiarize yourself with the fundamental concepts of Docker and Terraform. Ensure you can define terms like containers, images, infrastructure as code, etc.\\n\\n2. **Hands-on Practice**: Actively engage with the tools. Set up Docker and Terraform on your local machine and practice with sample projects. Build small applications using Docker and try to manage infrastructure using Terraform.\\n\\n3. **Follow Tutorials**: Utilize available tutorials that guide you through common tasks and configurations within Docker and Terraform.\\n\\n4. **Engage with the Community**: Participate in forums, discussion groups, or study sessions with other students. Sharing experiences and troubleshooting together can enhance your learning.\\n\\n5. **Learn from Errors**: Don't be discouraged by errors or challenges faced in the module. Each obstacle is a learning opportunity. Review error messages carefully and seek help if needed.\\n\\n6. **Stay Organized**: Keep track of your projects, notes, and any issues you encounter. An organized approach can make it easier to review concepts and debug problems later.\\n\\n7. **Time Management**: Allocate regular study times for Module 1. Consistent practice will help reinforce learning and prevent last-minute cramming.\\n\\n8. **Review Course Material**: Make sure to review provided materials, such as lectures, reading assignments, or additional resources, as these will often highlight key areas of focus for the module.\\n\\nBy combining theoretical knowledge with practical experience and community engagement, you can set a strong foundation for success in Module 1.\",\n",
      "  \"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"what do I need to do to be successful at module 1?\"\n",
    "\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []\n",
    "\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    print(f'ITERATION #{iteration}...')\n",
    "\n",
    "    context = build_context(search_results)\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        search_queries=\"\\n\".join(search_queries),\n",
    "        previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "        max_iterations=3,\n",
    "        iteration_number=iteration\n",
    "    )\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(json.dumps(answer, indent=2))\n",
    "\n",
    "    previous_actions.append(answer)\n",
    "\n",
    "    action = answer['action']\n",
    "    if action != 'SEARCH':\n",
    "        break\n",
    "\n",
    "    keywords = answer['keywords']\n",
    "    search_queries = list(set(search_queries) | set(keywords))\n",
    "    \n",
    "    for k in keywords:\n",
    "        res = search(k)\n",
    "        search_results.extend(res)\n",
    "\n",
    "    search_results = dedup(search_results)\n",
    "    \n",
    "    iteration = iteration + 1\n",
    "    if iteration >= 4:\n",
    "        break\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a2ee162d-15ec-4636-8b6b-8a1265d72ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_search(question):\n",
    "    search_queries = []\n",
    "    search_results = []\n",
    "    previous_actions = []\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        print(f'ITERATION #{iteration}...')\n",
    "    \n",
    "        context = build_context(search_results)\n",
    "        prompt = prompt_template.format(\n",
    "            question=question,\n",
    "            context=context,\n",
    "            search_queries=\"\\n\".join(search_queries),\n",
    "            previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "            max_iterations=3,\n",
    "            iteration_number=iteration\n",
    "        )\n",
    "    \n",
    "        print(prompt)\n",
    "    \n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(json.dumps(answer, indent=2))\n",
    "\n",
    "        previous_actions.append(answer)\n",
    "    \n",
    "        action = answer['action']\n",
    "        if action != 'SEARCH':\n",
    "            break\n",
    "    \n",
    "        keywords = answer['keywords']\n",
    "        search_queries = list(set(search_queries) | set(keywords))\n",
    "\n",
    "        for k in keywords:\n",
    "            res = search(k)\n",
    "            search_results.extend(res)\n",
    "    \n",
    "        search_results = dedup(search_results)\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "        if iteration >= 4:\n",
    "            break\n",
    "    \n",
    "        print()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7de2df44-d9fd-4b9c-baa1-ef462c4d8701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"I need to gather information on course preparation strategies and resources from the FAQ database to provide comprehensive guidance to the student on how to prepare for the course.\",\n",
      "  \"keywords\": [\n",
      "    \"course preparation\",\n",
      "    \"how to prepare for classes\",\n",
      "    \"study tips for courses\",\n",
      "    \"resources for course success\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "resources for course success\n",
      "study tips for courses\n",
      "course preparation\n",
      "how to prepare for classes\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Spark Cloud Storage connector\n",
      "answer: Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\n",
      "There’s a few extra steps to go into reading from GCS with PySpark\n",
      "1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\n",
      "As the name implies, this .jar file is what essentially connects PySpark with your GCS\n",
      "2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\n",
      "3.) In your Python script, there are a few extra classes you’ll have to import:\n",
      "import pyspark\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.conf import SparkConf\n",
      "from pyspark.context import SparkContext\n",
      "4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\n",
      "conf = SparkConf() \\\n",
      ".setMaster('local[*]') \\\n",
      ".setAppName('test') \\\n",
      ".set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
      ".set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
      ".set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n",
      "sc = SparkContext(conf=conf)\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
      "5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\n",
      "spark = SparkSession.builder \\\n",
      ".config(conf=sc.getConf()) \\\n",
      ".getOrCreate()\n",
      "6.) Finally, you’re able to read your files straight from GCS!\n",
      "df_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Java Kafka: Tests are not picked up in VSCode\n",
      "answer: Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\n",
      "Solution:\n",
      "(Source)\n",
      "VS Code\n",
      "→ Explorer (first icon on the left navigation bar)\n",
      "→ JAVA PROJECTS (bottom collapsable)\n",
      "→  icon next in the rightmost position to JAVA PROJECTS\n",
      "→  clean Workspace\n",
      "→ Confirm by clicking Reload and Delete\n",
      "Now you will be able to see the triangle icon next to each test like what you normally see in python tests.\n",
      "E.g.:\n",
      "You can also add classes and packages in this window instead of creating files in the project directory\n",
      "\n",
      "section: Module 3: Data Warehousing\n",
      "question: GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?\n",
      "answer: Check the Schema\n",
      "You might have a wrong formatting\n",
      "Try to upload the CSV.GZ files without formatting or going through pandas via wget\n",
      "See this Slack conversation for helpful tips\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n",
      "answer: If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\n",
      "For Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\bin” to the PATH variable.\n",
      "Additional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - how many Zoomcamps in a year?\n",
      "answer: There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\n",
      "Data-Engineering (Jan - Apr)\n",
      "MLOps (May - Aug)\n",
      "Machine Learning (Sep - Jan)\n",
      "There's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\n",
      "They follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?\n",
      "answer: When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\n",
      "Go to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing\n",
      "answer: Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv\n",
      "\n",
      "section: General course-related questions\n",
      "question: Any books or additional resources you recommend?\n",
      "answer: Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"I need to gather information on course preparation strategies and resources from the FAQ database to provide comprehensive guidance to the student on how to prepare for the course.\", \"keywords\": [\"course preparation\", \"how to prepare for classes\", \"study tips for courses\", \"resources for course success\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"I need to find more specific guidance on the necessary preparations and resources that could help with course readiness since the previous search did not yield any direct information about preparation methods or strategies.\",\n",
      "  \"keywords\": [\n",
      "    \"preparing for a course\",\n",
      "    \"successful study habits\",\n",
      "    \"course success strategies\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #2...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "resources for course success\n",
      "study tips for courses\n",
      "successful study habits\n",
      "preparing for a course\n",
      "course success strategies\n",
      "course preparation\n",
      "how to prepare for classes\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Spark Cloud Storage connector\n",
      "answer: Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\n",
      "There’s a few extra steps to go into reading from GCS with PySpark\n",
      "1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\n",
      "As the name implies, this .jar file is what essentially connects PySpark with your GCS\n",
      "2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\n",
      "3.) In your Python script, there are a few extra classes you’ll have to import:\n",
      "import pyspark\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.conf import SparkConf\n",
      "from pyspark.context import SparkContext\n",
      "4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\n",
      "conf = SparkConf() \\\n",
      ".setMaster('local[*]') \\\n",
      ".setAppName('test') \\\n",
      ".set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
      ".set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
      ".set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n",
      "sc = SparkContext(conf=conf)\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
      "5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\n",
      "spark = SparkSession.builder \\\n",
      ".config(conf=sc.getConf()) \\\n",
      ".getOrCreate()\n",
      "6.) Finally, you’re able to read your files straight from GCS!\n",
      "df_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Java Kafka: Tests are not picked up in VSCode\n",
      "answer: Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\n",
      "Solution:\n",
      "(Source)\n",
      "VS Code\n",
      "→ Explorer (first icon on the left navigation bar)\n",
      "→ JAVA PROJECTS (bottom collapsable)\n",
      "→  icon next in the rightmost position to JAVA PROJECTS\n",
      "→  clean Workspace\n",
      "→ Confirm by clicking Reload and Delete\n",
      "Now you will be able to see the triangle icon next to each test like what you normally see in python tests.\n",
      "E.g.:\n",
      "You can also add classes and packages in this window instead of creating files in the project directory\n",
      "\n",
      "section: Module 3: Data Warehousing\n",
      "question: GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?\n",
      "answer: Check the Schema\n",
      "You might have a wrong formatting\n",
      "Try to upload the CSV.GZ files without formatting or going through pandas via wget\n",
      "See this Slack conversation for helpful tips\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n",
      "answer: If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\n",
      "For Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\bin” to the PATH variable.\n",
      "Additional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - how many Zoomcamps in a year?\n",
      "answer: There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\n",
      "Data-Engineering (Jan - Apr)\n",
      "MLOps (May - Aug)\n",
      "Machine Learning (Sep - Jan)\n",
      "There's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\n",
      "They follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?\n",
      "answer: When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\n",
      "Go to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing\n",
      "answer: Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv\n",
      "\n",
      "section: General course-related questions\n",
      "question: Any books or additional resources you recommend?\n",
      "answer: Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - docker pull dbpage\n",
      "answer: Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\n",
      "IF the repository is public, the fetch and download happens without any issue whatsoever.\n",
      "For instance:\n",
      "docker pull postgres:13\n",
      "docker pull dpage/pgadmin4\n",
      "BE ADVISED:\n",
      "\n",
      "The Docker Images we'll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\n",
      "\n",
      "Meaning: you are NOT required to perform a docker login to fetch them. \n",
      "\n",
      "So if you get the message above saying \"docker login': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\n",
      "\n",
      "For instance:\n",
      "$ docker pull dbpage/pgadmin4\n",
      "Will throw that exception telling you \"repository does not exist or may require 'docker login'\n",
      "Error response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \n",
      "may require 'docker login': denied: requested access to the resource is denied\n",
      "But that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\n",
      "How to fix it:\n",
      "$ docker pull dpage/pgadmin4\n",
      "EXTRA NOTES:\n",
      "In the real world, occasionally, when you're working for a company or closed organisation, the Docker image you're trying to fetch might be under a private repo that your DockerHub Username was granted access to.\n",
      "For which cases, you must first execute:\n",
      "$ docker login\n",
      "Fill in the details of your username and password.\n",
      "And only then perform the `docker pull` against that private repository\n",
      "Why am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\n",
      "Issue Description:\n",
      "When attempting to run a Docker command similar to the one below:\n",
      "docker run -it \\\n",
      "-e POSTGRES_USER=\"root\" \\\n",
      "-e POSTGRES_PASSWORD=\"root\" \\\n",
      "-e POSTGRES_DB=\"ny_taxi\" \\\n",
      "-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n",
      "-p 5432:5432 \\mount\n",
      "postgres:13\n",
      "You encounter the error message:\n",
      "docker: Error response from daemon: error while creating mount source path '/path/to/ny_taxi_postgres_data': chown /path/to/ny_taxi_postgres_data: permission denied.\n",
      "Solution:\n",
      "1- Stop Rancher Desktop:\n",
      "If you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\n",
      "2- Install Docker Desktop:\n",
      "Install Docker Desktop, ensuring that it is properly configured and has the required permissions.\n",
      "2-Retry Docker Command:\n",
      "Run the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\n",
      "Note: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"I need to gather information on course preparation strategies and resources from the FAQ database to provide comprehensive guidance to the student on how to prepare for the course.\", \"keywords\": [\"course preparation\", \"how to prepare for classes\", \"study tips for courses\", \"resources for course success\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"I need to find more specific guidance on the necessary preparations and resources that could help with course readiness since the previous search did not yield any direct information about preparation methods or strategies.\", \"keywords\": [\"preparing for a course\", \"successful study habits\", \"course success strategies\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"I still don\\u2019t have specific details on effective study habits and successful strategies for preparing for this particular course. This search will help gather more targeted information that can assist the student in preparing for their studies effectively.\",\n",
      "  \"keywords\": [\n",
      "    \"effective study habits\",\n",
      "    \"course preparation tips\",\n",
      "    \"success strategies for courses\",\n",
      "    \"how to be successful in a course\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #3...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 3. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "how to be successful in a course\n",
      "resources for course success\n",
      "success strategies for courses\n",
      "successful study habits\n",
      "preparing for a course\n",
      "course success strategies\n",
      "study tips for courses\n",
      "how to prepare for classes\n",
      "course preparation tips\n",
      "effective study habits\n",
      "course preparation\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Spark Cloud Storage connector\n",
      "answer: Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\n",
      "There’s a few extra steps to go into reading from GCS with PySpark\n",
      "1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\n",
      "As the name implies, this .jar file is what essentially connects PySpark with your GCS\n",
      "2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\n",
      "3.) In your Python script, there are a few extra classes you’ll have to import:\n",
      "import pyspark\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.conf import SparkConf\n",
      "from pyspark.context import SparkContext\n",
      "4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\n",
      "conf = SparkConf() \\\n",
      ".setMaster('local[*]') \\\n",
      ".setAppName('test') \\\n",
      ".set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
      ".set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
      ".set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n",
      "sc = SparkContext(conf=conf)\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
      "5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\n",
      "spark = SparkSession.builder \\\n",
      ".config(conf=sc.getConf()) \\\n",
      ".getOrCreate()\n",
      "6.) Finally, you’re able to read your files straight from GCS!\n",
      "df_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Java Kafka: Tests are not picked up in VSCode\n",
      "answer: Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\n",
      "Solution:\n",
      "(Source)\n",
      "VS Code\n",
      "→ Explorer (first icon on the left navigation bar)\n",
      "→ JAVA PROJECTS (bottom collapsable)\n",
      "→  icon next in the rightmost position to JAVA PROJECTS\n",
      "→  clean Workspace\n",
      "→ Confirm by clicking Reload and Delete\n",
      "Now you will be able to see the triangle icon next to each test like what you normally see in python tests.\n",
      "E.g.:\n",
      "You can also add classes and packages in this window instead of creating files in the project directory\n",
      "\n",
      "section: Module 3: Data Warehousing\n",
      "question: GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?\n",
      "answer: Check the Schema\n",
      "You might have a wrong formatting\n",
      "Try to upload the CSV.GZ files without formatting or going through pandas via wget\n",
      "See this Slack conversation for helpful tips\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n",
      "answer: If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\n",
      "For Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\bin” to the PATH variable.\n",
      "Additional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - how many Zoomcamps in a year?\n",
      "answer: There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\n",
      "Data-Engineering (Jan - Apr)\n",
      "MLOps (May - Aug)\n",
      "Machine Learning (Sep - Jan)\n",
      "There's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\n",
      "They follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?\n",
      "answer: When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\n",
      "Go to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing\n",
      "answer: Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv\n",
      "\n",
      "section: General course-related questions\n",
      "question: Any books or additional resources you recommend?\n",
      "answer: Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - docker pull dbpage\n",
      "answer: Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\n",
      "IF the repository is public, the fetch and download happens without any issue whatsoever.\n",
      "For instance:\n",
      "docker pull postgres:13\n",
      "docker pull dpage/pgadmin4\n",
      "BE ADVISED:\n",
      "\n",
      "The Docker Images we'll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\n",
      "\n",
      "Meaning: you are NOT required to perform a docker login to fetch them. \n",
      "\n",
      "So if you get the message above saying \"docker login': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\n",
      "\n",
      "For instance:\n",
      "$ docker pull dbpage/pgadmin4\n",
      "Will throw that exception telling you \"repository does not exist or may require 'docker login'\n",
      "Error response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \n",
      "may require 'docker login': denied: requested access to the resource is denied\n",
      "But that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\n",
      "How to fix it:\n",
      "$ docker pull dpage/pgadmin4\n",
      "EXTRA NOTES:\n",
      "In the real world, occasionally, when you're working for a company or closed organisation, the Docker image you're trying to fetch might be under a private repo that your DockerHub Username was granted access to.\n",
      "For which cases, you must first execute:\n",
      "$ docker login\n",
      "Fill in the details of your username and password.\n",
      "And only then perform the `docker pull` against that private repository\n",
      "Why am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\n",
      "Issue Description:\n",
      "When attempting to run a Docker command similar to the one below:\n",
      "docker run -it \\\n",
      "-e POSTGRES_USER=\"root\" \\\n",
      "-e POSTGRES_PASSWORD=\"root\" \\\n",
      "-e POSTGRES_DB=\"ny_taxi\" \\\n",
      "-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n",
      "-p 5432:5432 \\mount\n",
      "postgres:13\n",
      "You encounter the error message:\n",
      "docker: Error response from daemon: error while creating mount source path '/path/to/ny_taxi_postgres_data': chown /path/to/ny_taxi_postgres_data: permission denied.\n",
      "Solution:\n",
      "1- Stop Rancher Desktop:\n",
      "If you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\n",
      "2- Install Docker Desktop:\n",
      "Install Docker Desktop, ensuring that it is properly configured and has the required permissions.\n",
      "2-Retry Docker Command:\n",
      "Run the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\n",
      "Note: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"I need to gather information on course preparation strategies and resources from the FAQ database to provide comprehensive guidance to the student on how to prepare for the course.\", \"keywords\": [\"course preparation\", \"how to prepare for classes\", \"study tips for courses\", \"resources for course success\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"I need to find more specific guidance on the necessary preparations and resources that could help with course readiness since the previous search did not yield any direct information about preparation methods or strategies.\", \"keywords\": [\"preparing for a course\", \"successful study habits\", \"course success strategies\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"I still don\\u2019t have specific details on effective study habits and successful strategies for preparing for this particular course. This search will help gather more targeted information that can assist the student in preparing for their studies effectively.\", \"keywords\": [\"effective study habits\", \"course preparation tips\", \"success strategies for courses\", \"how to be successful in a course\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER\",\n",
      "  \"answer\": \"To prepare for the course, consider the following strategies: \\n1. **Register Early**: Make sure you register for the course before it starts to gain access to all materials. \\n2. **Join Communication Channels**: Subscribe to the course public Google Calendar, and join the Telegram channel for announcements to stay updated.\\n3. **Set Up Required Tools**: Familiarize yourself with Git/GitHub as mentioned in the FAQs, since coding and collaboration via repositories will be necessary. Watch tutorials if you're unsure how to clone repositories or set up your environment.\\n4. **Review Course Materials**: Since materials will be available after the course finishes, you can start familiarizing yourself with them in advance. \\n5. **Participate Actively**: Engage in live sessions and peer reviews to understand concepts better, as interaction enhances learning.\\n6. **Study Habits**: Develop a consistent study schedule and utilize additional resources. For example, check out books and materials in the document mentioned in the FAQ for further insight into data engineering topics.\\n7. **Seek Help When Needed**: Don\\u2019t hesitate to reach out through Slack or course channels for questions or guidance during your preparation and learning process.\\nThese practices will set a solid foundation for your success in the course.\",\n",
      "  \"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To prepare for the course, consider the following strategies: \\n1. **Register Early**: Make sure you register for the course before it starts to gain access to all materials. \\n2. **Join Communication Channels**: Subscribe to the course public Google Calendar, and join the Telegram channel for announcements to stay updated.\\n3. **Set Up Required Tools**: Familiarize yourself with Git/GitHub as mentioned in the FAQs, since coding and collaboration via repositories will be necessary. Watch tutorials if you're unsure how to clone repositories or set up your environment.\\n4. **Review Course Materials**: Since materials will be available after the course finishes, you can start familiarizing yourself with them in advance. \\n5. **Participate Actively**: Engage in live sessions and peer reviews to understand concepts better, as interaction enhances learning.\\n6. **Study Habits**: Develop a consistent study schedule and utilize additional resources. For example, check out books and materials in the document mentioned in the FAQ for further insight into data engineering topics.\\n7. **Seek Help When Needed**: Don’t hesitate to reach out through Slack or course channels for questions or guidance during your preparation and learning process.\\nThese practices will set a solid foundation for your success in the course.\",\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_search('how do I prepare for the course?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "66788038-148f-4da1-adfe-22de6e1773c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T07:18:49.782329Z",
     "start_time": "2025-07-16T07:18:49.779952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'ANSWER', 'answer': \"To be successful in Module 1: Docker and Terraform, here are some general tips that can help:\\n\\n1. **Understand the Basics**: Familiarize yourself with the fundamental concepts of Docker and Terraform. Ensure you can define terms like containers, images, infrastructure as code, etc.\\n\\n2. **Hands-on Practice**: Actively engage with the tools. Set up Docker and Terraform on your local machine and practice with sample projects. Build small applications using Docker and try to manage infrastructure using Terraform.\\n\\n3. **Follow Tutorials**: Utilize available tutorials that guide you through common tasks and configurations within Docker and Terraform.\\n\\n4. **Engage with the Community**: Participate in forums, discussion groups, or study sessions with other students. Sharing experiences and troubleshooting together can enhance your learning.\\n\\n5. **Learn from Errors**: Don't be discouraged by errors or challenges faced in the module. Each obstacle is a learning opportunity. Review error messages carefully and seek help if needed.\\n\\n6. **Stay Organized**: Keep track of your projects, notes, and any issues you encounter. An organized approach can make it easier to review concepts and debug problems later.\\n\\n7. **Time Management**: Allocate regular study times for Module 1. Consistent practice will help reinforce learning and prevent last-minute cramming.\\n\\n8. **Review Course Material**: Make sure to review provided materials, such as lectures, reading assignments, or additional resources, as these will often highlight key areas of focus for the module.\\n\\nBy combining theoretical knowledge with practical experience and community engagement, you can set a strong foundation for success in Module 1.\", 'source': 'OWN_KNOWLEDGE'}\n"
     ]
    }
   ],
   "source": [
    "print(globals()['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353d1d5-d7c9-4f27-b12e-3dc51d27be85",
   "metadata": {},
   "source": [
    "## Tools (function calling)\n",
    "\n",
    "https://platform.openai.com/docs/guides/function-calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e854ad2f-4bfa-48fa-bab1-f925afbdc0cc",
   "metadata": {},
   "source": [
    "    def search(query):\n",
    "        boost = {'question': 3.0, 'section': 0.5}\n",
    "    \n",
    "        results = index.search(\n",
    "            query=query,\n",
    "            filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "            boost_dict=boost,\n",
    "            num_results=5,\n",
    "            output_ids=True\n",
    "        )\n",
    "    \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "28133938-0af4-4e9d-825f-4f44d42b5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "df28b463-990a-482d-ba49-c55403bbf2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c0ac6e7d-bbc1-40fd-8b34-249f9805f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I do well in module 1?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c48fb94b-5a14-4d7c-a695-f9672c846ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "81502be5-0e65-4364-8c64-9dee771f5a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_687765102a3881a1a665a55fdc01ce2f06cdef125cb6f434', created_at=1752655120.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFunctionToolCall(arguments='{\"query\":\"how to do well in module 1\"}', call_id='call_BdJd6v1O9d7fEq8hGzdyfQka', name='search', type='function_call', id='fc_68776510c1fc81a19702560302a0640e06cdef125cb6f434', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='search', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'Search query text to look up in the course FAQ.'}}, 'required': ['query'], 'additionalProperties': False}, strict=True, type='function', description='Search the FAQ database')], top_p=1.0, background=False, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=84, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=21, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=105), user=None, store=True)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "781242a3-c3b0-4083-9adc-d88aa8797885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"query\":\"how to do well in module 1\"}', call_id='call_BdJd6v1O9d7fEq8hGzdyfQka', name='search', type='function_call', id='fc_68776510c1fc81a19702560302a0640e06cdef125cb6f434', status='completed')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a8c5723a-cbf4-4e71-a07b-73ca4016c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.choices[0].message.content\n",
    "calls = response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7759e3c4-835a-4fee-9f15-30b19706ace8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseFunctionToolCall(arguments='{\"query\":\"how to do well in module 1\"}', call_id='call_BdJd6v1O9d7fEq8hGzdyfQka', name='search', type='function_call', id='fc_68776510c1fc81a19702560302a0640e06cdef125cb6f434', status='completed')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call = calls[0]\n",
    "call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e14a75d1-981e-44f7-bc2b-bf24509d11f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call_BdJd6v1O9d7fEq8hGzdyfQka'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call.call_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4226a3ff-4cd0-4e2f-9c22-96c7a035ba4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_name = call.name\n",
    "f_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c6fb3740-089f-4c26-a3bc-87dbc9e145c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'how to do well in module 1'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments = json.loads(call.arguments)\n",
    "arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "11d47c39-2688-432a-afc8-b7d481487123",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = locals()[f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ac44a26e-f5b3-4e7f-997a-ba709d0bef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = f(**arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "81342abc-e9f3-4fa5-a546-fd317c292b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"Issue:\\ne\\u2026\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the \\u201c ModuleNotFoundError: No module named 'psycopg2' \\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
      "    \"section\": \"Module 1: Docker and Terraform\",\n",
      "    \"question\": \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 112\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
      "    \"section\": \"Module 4: analytics engineering with dbt\",\n",
      "    \"question\": \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 299\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \\\"TypeError: 'module' object is not callable\\\"\\nSolution:\\nconn_string = \\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\"\\nengine = create_engine(conn_string)\",\n",
      "    \"section\": \"Module 1: Docker and Terraform\",\n",
      "    \"question\": \"Python - SQLALchemy - TypeError 'module' object is not callable\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 124\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Error raised during the jupyter notebook\\u2019s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module \\u201cpsycopg2\\u201d. Can be installed by Conda or pip.\",\n",
      "    \"section\": \"Module 1: Docker and Terraform\",\n",
      "    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 125\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\u201d\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\"` appropriately.\\nAdditionally, you can check for the version of \\u2018py4j\\u2019 of the spark you\\u2019re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\n",
      "    \"section\": \"Module 5: pyspark\",\n",
      "    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 323\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "search_results = json.dumps(results, indent=2)\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "572bc1e3-51b0-4dc1-9030-131a9a057c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages.append(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f1099da2-9710-4d24-865f-9d59ecfc9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages.append({\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": search_results,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7b41e545-626f-471c-b9a7-4d74efc2f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5576052f-7244-4b3c-ac18-3e3d305739dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = response.output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d91eb0af-cf09-410e-93ca-407aebf551b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To do well in Module 1 of your course, which focuses on Docker and Terraform, consider the following strategies:\n",
      "\n",
      "1. **Understand Key Concepts**:\n",
      "   - Familiarize yourself with Docker and Terraform fundamentals. Ensure you understand how containers work, as well as the basics of Infrastructure as Code (IaC) using Terraform.\n",
      "\n",
      "2. **Hands-on Practice**:\n",
      "   - Set up Docker and create your first container. Practice commands like `docker run`, `docker-compose`, and learn to manage images.\n",
      "   - Work through examples on Terraform to create resources in your cloud environment. Start with simple infrastructure setups and gradually move to more complex configurations.\n",
      "\n",
      "3. **Follow Course Materials**:\n",
      "   - Carefully read the course materials, including any README files and documentation related to the exercises in Module 1. \n",
      "\n",
      "4. **Solve Common Issues**:\n",
      "   - Be prepared to troubleshoot common errors. For instance, you might encounter the \"ModuleNotFoundError\" related to libraries like `psycopg2`. Ensure you install necessary packages using:\n",
      "     ```bash\n",
      "     pip install psycopg2-binary\n",
      "     ```\n",
      "   - If you face issues, check your installation and consider upgrading your installation tools like pip or conda if necessary.\n",
      "\n",
      "5. **Engage with the Community**:\n",
      "   - Don’t hesitate to reach out in forums or study groups if you encounter challenges. Collaborating with peers can enhance your understanding.\n",
      "\n",
      "6. **Review and Reflect**:\n",
      "   - Regularly review what you have learned. Consider writing notes or creating flashcards for key concepts to reinforce your memory.\n",
      "\n",
      "7. **Practice, Practice, Practice**:\n",
      "   - Apply what you've learned by building small projects or setting up complex scenarios that utilize both Docker and Terraform.\n",
      "\n",
      "By maintaining a proactive learning attitude and engaging deeply with the course materials and practices, you'll enhance your chances of excelling in Module 1. Good luck!\n"
     ]
    }
   ],
   "source": [
    "print(r.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e6195a9d-f737-415e-bfb5-a413d449c94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'message'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1ee391d2-8b2a-4b8c-85f7-6a56d5fa3f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'function_call'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call.type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5746ea-4d80-4eee-8d16-95305cc66ebc",
   "metadata": {},
   "source": [
    "### Multiple calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fd69ffd0-2a4b-454e-87bd-2ff976d61a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_call(tool_call_response):\n",
    "    function_name = tool_call_response.name\n",
    "    arguments = json.loads(tool_call_response.arguments)\n",
    "\n",
    "    f = globals()[function_name]\n",
    "    result = f(**arguments)\n",
    "\n",
    "    return {\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": tool_call_response.call_id,\n",
    "        \"output\": json.dumps(result, indent=2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f5567651-5716-4d47-9002-d347bd69b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "If you look up something in FAQ, convert the student question into multiple queries.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e2176d6f-8d0b-4e12-8b2d-64bbf9cc9961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_687765b5c514819ca21c3a98118ea1c604ab1cc29eab414c', created_at=1752655285.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFunctionToolCall(arguments='{\"query\":\"module 1 tips for success\"}', call_id='call_S4GeGhDIlnLBGRrGaafLw9L0', name='search', type='function_call', id='fc_687765b66b48819c93e333dc4165a9ee04ab1cc29eab414c', status='completed'), ResponseFunctionToolCall(arguments='{\"query\":\"module 1 study strategies\"}', call_id='call_ezam4h3XgQxQ2wJXN9m8pRIh', name='search', type='function_call', id='fc_687765b6cac4819cb1fcd01ecf3b02e904ab1cc29eab414c', status='completed'), ResponseFunctionToolCall(arguments='{\"query\":\"module 1 assessment overview\"}', call_id='call_FNZnwxDzRx1m0JouZtYJUviZ', name='search', type='function_call', id='fc_687765b71e40819c86f6006b3abee77904ab1cc29eab414c', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='search', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'Search query text to look up in the course FAQ.'}}, 'required': ['query'], 'additionalProperties': False}, strict=True, type='function', description='Search the FAQ database')], top_p=1.0, background=False, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=99, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=69, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=168), user=None, store=True)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ac9a762c-1836-4573-b0e5-c927e2fb47b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call\n",
      "function_call\n",
      "function_call\n"
     ]
    }
   ],
   "source": [
    "for entry in response.output:\n",
    "    chat_messages.append(entry)\n",
    "    print(entry.type)\n",
    "\n",
    "    if entry.type == 'function_call':      \n",
    "        result = do_call(entry)\n",
    "        chat_messages.append(result)\n",
    "    elif entry.type == 'message':\n",
    "        print(entry.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "caee8500-ffc9-4aa8-9f88-283f3a5d9446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message\n",
      "\n",
      "To excel in Module 1, here are some key strategies and tips:\n",
      "\n",
      "### 1. Understand the Course Material\n",
      "- **Review Lecture Notes**: Make sure to review all lecture slides and notes provided. Understanding the foundational concepts is crucial.\n",
      "- **Read Recommended Resources**: Go through any additional readings suggested by the instructor.\n",
      "\n",
      "### 2. Practical Exercises\n",
      "- **Hands-on Practice**: Engage in any coding exercises or practical implementations given in the module, especially with tools like Docker and Terraform.\n",
      "- **Code Along with Examples**: Follow along with any coding demonstrations provided in the lectures.\n",
      "\n",
      "### 3. Troubleshooting Common Issues\n",
      "- **Install Necessary Packages**: Ensure you have all required packages installed. For example, if you encounter a `ModuleNotFoundError` for `psycopg2`, install it using:\n",
      "  ```bash\n",
      "  pip install psycopg2-binary\n",
      "  ```\n",
      "- **Keep Software Updated**: Regularly update your installations, using:\n",
      "  ```bash\n",
      "  pip install --upgrade psycopg2-binary\n",
      "  ```\n",
      "  to ensure compatibility.\n",
      "\n",
      "### 4. Utilize Tools and Resources\n",
      "- **Use Jupyter Notebooks**: If using Jupyter Notebooks, remember to initialize your Spark environment:\n",
      "  ```python\n",
      "  !pip install findspark\n",
      "  import findspark\n",
      "  findspark.init()\n",
      "  ```\n",
      "- **Optional Dependencies**: If you run into module errors, you may need to install optional dependencies specific to your environment.\n",
      "\n",
      "### 5. Participate in Discussions\n",
      "- **Engage with Peers**: Join discussion forums or groups related to the course. This can greatly enhance your understanding through peer support.\n",
      "- **Ask Questions**: Don’t hesitate to reach out to instructors or peers if you encounter unclear concepts.\n",
      "\n",
      "### 6. Assessment Preparation\n",
      "- **Understand Assessment Formats**: Familiarize yourself with the types of assessments that will be in Module 1, and practice similar problems.\n",
      "- **Study Past Examples**: Review example assessments if available to structure your study plan.\n",
      "\n",
      "By following these strategies, you'll be better prepared to tackle the challenges in Module 1 and perform well. If you need specific assistance or have any questions about the course content, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "for entry in response.output:\n",
    "    chat_messages.append(entry)\n",
    "    print(entry.type)\n",
    "    print()\n",
    "\n",
    "    if entry.type == 'function_call':      \n",
    "        result = do_call(entry)\n",
    "        chat_messages.append(result)\n",
    "    elif entry.type == 'message':\n",
    "        print(entry.content[0].text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52fc54b-10a1-4aa8-88e5-dab893e612ee",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722923e7-9fe3-4561-bf3f-682de839a6ac",
   "metadata": {},
   "source": [
    "Have two loops:\n",
    "\n",
    "- First is the main Q&A loop - ask question, get back the answer\n",
    "- Second is the request loop - send requests until there's a message reply from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "87be0bfd-7fd8-4102-b05d-f6ef4fd48fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "When using FAQ, perform deep topic exploration: make one request to FAQ,\n",
    "and then based on the results, make more requests.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9615aa6a-4103-4e04-b327-f2009d8214bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " How do I do my best in Module 2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"best practices for Module 2\"}', call_id='call_MhTI4UfXHYoiEQQCzlOWiWqB', name='search', type='function_call', id='fc_6877674dd42481a0b2c24f466a139c540d8a7f064d673908', status='completed')\n",
      "\n",
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"Module 2 best practices study tips\"}', call_id='call_9lcpW6gGa8I6Ckj0kzFDaSi8', name='search', type='function_call', id='fc_6877674eddd081a0b7b77f73e41221f70d8a7f064d673908', status='completed')\n",
      "\n",
      "To excel in Module 2, here are some strategies you can consider:\n",
      "\n",
      "1. **Understand the Content**: Make sure you have a clear understanding of the topics covered in the module. This might include tools or technologies introduced in this module. If there are any best practices mentioned, such as in the Docker documentation, review those thoroughly.\n",
      "\n",
      "2. **Hands-On Practice**: Engage in practical exercises. Try to replicate examples and work through any provided assignments or lab exercises. This practice can solidify your understanding and help you apply theoretical concepts.\n",
      "\n",
      "3. **Utilize Resources**: Check out additional resources such as tutorials, videos, or forums related to Module 2. You may also find book references helpful.\n",
      "\n",
      "4. **Study Groups**: Consider forming or joining a study group with other students. Discussing topics with peers can provide new insights and enhance your learning experience.\n",
      "\n",
      "5. **Ask Questions**: Don’t hesitate to reach out for help if you’re struggling with specific concepts. Engaging with your instructors or teaching assistants can clarify doubts and enhance your understanding.\n",
      "\n",
      "6. **Feedback on Assignments**: If you have assignments or quizzes, ensure to review any feedback received carefully. Learning from your mistakes is one of the best ways to improve.\n",
      "\n",
      "7. **Time Management**: Allocate specific time slots for studying Module 2, and make sure to stick to this schedule to avoid last-minute cramming.\n",
      "\n",
      "While the details specific to Module 2 weren't available from the FAQ, these general tips are often applicable across different topics. If you have specific concepts or topics within Module 2 that you would like more detailed advice on, feel free to ask!\n",
      "\n",
      "What specific aspect of Module 2 do you feel you need the most help with?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 3. Utilize resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"recommended resources for Module 2\"}', call_id='call_WszplioFrMyx5UWjGfz4W73t', name='search', type='function_call', id='fc_687767868b4881a09cbfd1b2efd1ed820d8a7f064d673908', status='completed')\n",
      "\n",
      "Here are some recommended resources to help you excel in Module 2:\n",
      "\n",
      "1. **Official Documentation**: Make it a habit to refer to the official documentation of the tools or technologies covered in Module 2. This can provide detailed insights and examples not covered in course materials.\n",
      "\n",
      "2. **Online Courses and Tutorials**: Platforms like Coursera, Udacity, or edX may have supplementary courses that cover the same topics. This could provide different perspectives or use cases.\n",
      "\n",
      "3. **YouTube Tutorials**: There are many tutorial channels focusing on data engineering tools (like Kafka, Spark, etc.) which can reinforce your understanding through visual aids.\n",
      "\n",
      "4. **Forums and Community Support**: Engage in discussions on platforms like Stack Overflow, Reddit, or specialized forums related to the tools you're studying. You can ask questions and share insights.\n",
      "\n",
      "5. **GitHub Repositories**: Look for open-source projects on GitHub that utilize the tools in Module 2. Exploring real-world applications can enhance your practical understanding.\n",
      "\n",
      "6. **Reference Books**: Consider checking out specific books on data engineering or the technologies being taught in Module 2. They often have case studies and practical examples.\n",
      "\n",
      "7. **Peer Resources**: If your peers have found helpful resources (like articles or papers), ask them to share those with you. Collaborative learning can yield new insights.\n",
      "\n",
      "8. **Follow Relevant Blogs**: Many professionals write blogs about their experiences and best practices using these technologies. Following relevant blogs can keep you updated on trends and techniques.\n",
      "\n",
      "If you have any particular tools or topics within Module 2 that you're interested in finding more resources for, let me know!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " stop\n"
     ]
    }
   ],
   "source": [
    "while True: # main Q&A loop\n",
    "    question = input() # How do I do my best for module 1?\n",
    "    if question == 'stop':\n",
    "        break\n",
    "\n",
    "    message = {\"role\": \"user\", \"content\": question}\n",
    "    chat_messages.append(message)\n",
    "\n",
    "    while True: # request-response loop - query API till get a message\n",
    "        response = client.responses.create(\n",
    "            model='gpt-4o-mini',\n",
    "            input=chat_messages,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "        has_messages = False\n",
    "        \n",
    "        for entry in response.output:\n",
    "            chat_messages.append(entry)\n",
    "        \n",
    "            if entry.type == 'function_call':      \n",
    "                print('function_call:', entry)\n",
    "                print()\n",
    "                result = do_call(entry)\n",
    "                chat_messages.append(result)\n",
    "            elif entry.type == 'message':\n",
    "                print(entry.content[0].text)\n",
    "                print()\n",
    "                has_messages = True\n",
    "\n",
    "        if has_messages:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704058a-5a92-4b23-a9d9-6917a9dce950",
   "metadata": {},
   "source": [
    "Same using widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "adc6338b-b742-431b-ac53-080d575fbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import markdown # pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "491da276-42e8-4069-bc46-57fe2314c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I need a transcript of all the lecture videos in Module 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'display_function_call' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m     result \u001b[38;5;241m=\u001b[39m do_call(entry)\n\u001b[1;32m     39\u001b[0m     chat_messages\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m---> 40\u001b[0m     display_function_call(entry, result)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     43\u001b[0m     display_response(entry)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_function_call' is not defined"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "]\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    question = input()\n",
    "    if question.strip().lower() == 'stop':\n",
    "        print(\"Chat ended.\")\n",
    "        break\n",
    "    print()\n",
    "\n",
    "    message = {\"role\": \"user\", \"content\": question}\n",
    "    chat_messages.append(message)\n",
    "\n",
    "    while True:  # inner request loop\n",
    "        response = client.responses.create(\n",
    "            model='gpt-4o-mini',\n",
    "            input=chat_messages,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "        has_messages = False\n",
    "\n",
    "        for entry in response.output:\n",
    "            chat_messages.append(entry)\n",
    "\n",
    "            if entry.type == \"function_call\":\n",
    "                result = do_call(entry)\n",
    "                chat_messages.append(result)\n",
    "                display_function_call(entry, result)\n",
    "\n",
    "            elif entry.type == \"message\":\n",
    "                display_response(entry)\n",
    "                has_messages = True\n",
    "\n",
    "        if has_messages:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b17b9a-7e17-4d00-9d22-5c0b50102428",
   "metadata": {},
   "source": [
    "## Adding more tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2617748e-92a7-4425-8678-42f7d6416238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entry(question, answer):\n",
    "    doc = {\n",
    "        'question': question,\n",
    "        'text': answer,\n",
    "        'section': 'user added',\n",
    "        'course': 'data-engineering-zoomcamp'\n",
    "    }\n",
    "    index.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5a832e2b-6330-4e82-a223-5ae15b21c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_entry_description = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"add_entry\",\n",
    "    \"description\": \"Add an entry to the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The question to be added to the FAQ database\",\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The answer to the question\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"question\", \"answer\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a3aefc9e-ae3f-4a67-8984-cffaf0ab9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chat_assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "240044a0-58ea-4791-b050-5a0fd8142fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = chat_assistant.Tools()\n",
    "tools.add_tool(search, search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e7131fa5-0253-4038-a70d-e94d5e6a6fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'Search query text to look up in the course FAQ.'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.get_tools()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "04d92456-197c-497b-b291-c7a36e6822df",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_interface = chat_assistant.ChatInterface()\n",
    "\n",
    "chat = chat_assistant.ChatAssistant(\n",
    "    tools=tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b5bf443e-33b5-480d-ace8-d7091f9756ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: Can I still join the course?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"Can I still join the course?\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"Can I still join the course?\"}', call_id='call_otLrabnah44k9AUVfw9gcbZL', name='search', type='function_call', id='fc_68776c3ef89081a38529a19feddb144006149f84fa59d4ec', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Course - Can I still join the course after the start date?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 2\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"No, you can only get a certificate if you finish the course with a \\u201clive\\u201d cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Certificate - Can I follow the course in a self-paced mode and get a certificate?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 11\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Course - Can I follow the course after it finishes?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 7\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  \\u201cOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon\\u2019t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Course - When will the course start?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 0\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 3\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Yes, you can still join the course! Even if you haven't registered, you're eligible to submit homework assignments. Just keep in mind that there will be deadlines for turning in final projects, so it's best not to leave everything until the last minute.</p>\n",
       "<p>Are you interested in starting with the homework assignments?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: When is the deadline for the project?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"What is the deadline for the final pr...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"What is the deadline for the final project?\"}', call_id='call_EB8Mv1GBLmATXMWZZBGjCqcs', name='search', type='function_call', id='fc_68776c5ae31c81a39d19d72592cfe52106149f84fa59d4ec', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"Yes, you can use any tool you want for your project.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Can I use Airflow instead for my final project?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 32\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.\",\n",
       "    \"section\": \"Project\",\n",
       "    \"question\": \"How is my capstone project going to be evaluated?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 395\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"There is only ONE project for this Zoomcamp. You do not need to submit or create two projects. There are simply TWO chances to pass the course. You can use the Second Attempt if you a) fail the first attempt b) do not have the time due to other engagements such as holiday or sickness etc. to enter your project into the first attempt.\",\n",
       "    \"section\": \"Project\",\n",
       "    \"question\": \"Project 1 & Project 2\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 396\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"You will have two attempts for a project. If the first project deadline is over and you\\u2019re late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Project - What is Project Attemp #1 and Project Attempt #2 exactly?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 38\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Homework - What are homework and project deadlines?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 14\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>You can find the latest deadlines for the project <a href=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\">here</a>. It's important to keep an eye on announcements for any updates or extensions as well.</p>\n",
       "<p>Do you have a specific project idea in mind that you’d like to work on?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "chat.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "23c3e0db-b29e-4844-a0f9-0e3946cc2705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'Search query text to look up in the course FAQ.'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}},\n",
       " {'type': 'function',\n",
       "  'name': 'add_entry',\n",
       "  'description': 'Add an entry to the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'question': {'type': 'string',\n",
       "     'description': 'The question to be added to the FAQ database'},\n",
       "    'answer': {'type': 'string', 'description': 'The answer to the question'}},\n",
       "   'required': ['question', 'answer'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.add_tool(add_entry, add_entry_description)\n",
    "tools.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8c01727c-0f04-4788-bb6c-9954f05b8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = chat_assistant.ChatAssistant(\n",
    "    tools=tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "24c594de-990e-4692-b0f6-23af522b0b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: How do I fail in Module 1?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"fail Module 1\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"fail Module 1\"}', call_id='call_Y3hRMU4sBKJiALcv2BRT73zl', name='search', type='function_call', id='fc_68776cc7027481a39d6a365d42a632f700b103bb66f91c5c', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"Issue:\\ne\\u2026\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the \\u201c ModuleNotFoundError: No module named 'psycopg2' \\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 112\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
       "    \"section\": \"Module 4: analytics engineering with dbt\",\n",
       "    \"question\": \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 299\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\\\"Murray Hill\\\") & (new_final.b_zone==\\\"Midwood\\\")).show()\\nKrishna Anand\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 322\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\u201d\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\"` appropriately.\\nAdditionally, you can check for the version of \\u2018py4j\\u2019 of the spark you\\u2019re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 323\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Error raised during the jupyter notebook\\u2019s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module \\u201cpsycopg2\\u201d. Can be installed by Conda or pip.\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 125\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To fail in Module 1, you would likely need to demonstrate consistent poor performance in assessments, quizzes, and other evaluations. Specifically, the following actions could contribute to a failure:</p>\n",
       "<ol>\n",
       "<li><strong>Not Completing Assignments</strong>: Failing to submit any assignments or assessments for Module 1 would directly impact your grade.</li>\n",
       "<li><strong>Consistently Poor Performance</strong>: Scoring low grades on quizzes, exams, or practical assignments might lead to an unsuccessful completion of the module.</li>\n",
       "<li><strong>Disregarding Participation Requirements</strong>: Failing to engage in group discussions or activities, if required, could also lead to a lower overall evaluation.</li>\n",
       "<li><strong>Not Seeking Help</strong>: Ignoring feedback or failing to seek help when facing challenges could lead to continued misunderstanding of the course content.</li>\n",
       "</ol>\n",
       "<p>It's important to manage your time well, actively engage with the materials, and seek assistance when needed to avoid these pitfalls.</p>\n",
       "<p>Would you like tips on how to succeed in Module 1 instead?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: Add this back to the FAQ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>add_entry({\"question\":\"How do I fail in Module 1?\",\"answe...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"question\":\"How do I fail in Module 1?\",\"answer\":\"To fail in Module 1, you would likely need to demonstrate consistent poor performance in assessments, quizzes, and other evaluations. Specifically, the following actions could contribute to a failure: 1. Not Completing Assignments: Failing to submit any assignments or assessments for Module 1 would directly impact your grade. 2. Consistently Poor Performance: Scoring low grades on quizzes, exams, or practical assignments might lead to an unsuccessful completion of the module. 3. Disregarding Participation Requirements: Failing to engage in group discussions or activities, if required, could also lead to a lower overall evaluation. 4. Not Seeking Help: Ignoring feedback or failing to seek help when facing challenges could lead to continued misunderstanding of the course content. It\\'s important to manage your time well, actively engage with the materials, and seek assistance when needed to avoid these pitfalls.\"}', call_id='call_78Oy1HHg5wwrdRRVrpTJgH7A', name='add_entry', type='function_call', id='fc_68776cde3d5481a3bce9d11e439b567600b103bb66f91c5c', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>null</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>I've successfully added the information about how to fail in Module 1 to the FAQ. </p>\n",
       "<p>If you have any further questions or need more assistance, feel free to ask! Would you like help with strategies for success in Module 1?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: index[-1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>It looks like you might be referencing indexing, possibly in a programming context (like Python). The expression <code>index[-1]</code> typically refers to accessing the last item in a list or array.</p>\n",
       "<p>If you meant something specific by that or need help with a coding-related question, feel free to elaborate! Are you working on a particular programming task or concept?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "chat.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b66040ec-a146-47d7-aaec-53322100987a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How do I fail in Module 1?',\n",
       " 'text': \"To fail in Module 1, you would likely need to demonstrate consistent poor performance in assessments, quizzes, and other evaluations. Specifically, the following actions could contribute to a failure: 1. Not Completing Assignments: Failing to submit any assignments or assessments for Module 1 would directly impact your grade. 2. Consistently Poor Performance: Scoring low grades on quizzes, exams, or practical assignments might lead to an unsuccessful completion of the module. 3. Disregarding Participation Requirements: Failing to engage in group discussions or activities, if required, could also lead to a lower overall evaluation. 4. Not Seeking Help: Ignoring feedback or failing to seek help when facing challenges could lead to continued misunderstanding of the course content. It's important to manage your time well, actively engage with the materials, and seek assistance when needed to avoid these pitfalls.\",\n",
       " 'section': 'user added',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.docs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bc58924f-1ce4-4bfa-812e-2362c6958c5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic-ai\n",
      "  Downloading pydantic_ai-0.4.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic-ai-slim==0.4.2 (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading pydantic_ai_slim-0.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting eval-type-backport>=0.2.0 (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting griffe>=1.3.2 (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.28.1)\n",
      "Collecting opentelemetry-api>=1.28.0 (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pydantic-graph==0.4.2 (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading pydantic_graph-0.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pydantic>=2.10 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.10.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.4.1)\n",
      "Collecting anthropic>=0.52.0 (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading anthropic-0.57.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting boto3>=1.37.24 (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading boto3-1.39.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting argcomplete>=3.5.0 (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (3.0.43)\n",
      "Requirement already satisfied: rich>=13 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (14.0.0)\n",
      "Collecting cohere>=5.13.11 (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading cohere-5.16.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pydantic-evals==0.4.2 (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading pydantic_evals-0.4.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting google-genai>=1.24.0 (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading google_genai-1.25.0-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting groq>=0.19.0 (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading groq-0.30.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting mcp>=1.9.4 (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading mcp-1.11.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting mistralai>=1.2.5 (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading mistralai-1.9.2-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: openai>=1.76.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.93.3)\n",
      "Collecting google-auth>=2.36.0 (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.32.4)\n",
      "Requirement already satisfied: anyio>=0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-evals==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (4.9.0)\n",
      "Collecting logfire-api>=1.2.0 (from pydantic-evals==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading logfire_api-3.24.2-py3-none-any.whl.metadata (972 bytes)\n",
      "Collecting pyyaml>=6.0.2 (from pydantic-evals==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from anyio>=0->pydantic-evals==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (3.7)\n",
      "Requirement already satisfied: certifi in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from httpx>=0.27->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from httpx>=0.27->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic>=2.10->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic>=2.10->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.27.2)\n",
      "Collecting botocore<1.40.0,>=1.39.6 (from boto3>=1.37.24->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading botocore-1.39.6-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from boto3>=1.37.24->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.0.1)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3>=1.37.24->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from botocore<1.40.0,>=1.39.6->boto3>=1.37.24->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from botocore<1.40.0,>=1.39.6->boto3>=1.37.24->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.40.0,>=1.39.6->boto3>=1.37.24->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.16.0)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading fastavro-1.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.21.1)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.0.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.33.0)\n",
      "Requirement already satisfied: filelock in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (23.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (4.66.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth>=2.36.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.4.8)\n",
      "Collecting tenacity<9.0.0,>=8.2.3 (from google-genai>=1.24.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai>=1.24.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: colorama>=0.4 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from griffe>=1.3.2->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.4.6)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (4.24.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.0.20)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai)\n",
      "  Downloading sse_starlette-2.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: starlette>=0.27 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.45.3)\n",
      "Requirement already satisfied: uvicorn>=0.23.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.35.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.10.6)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.28.0->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (3.17.0)\n",
      "Requirement already satisfied: wcwidth in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from prompt-toolkit>=3->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.2.5)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-settings>=2.5.2->mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from rich>=13->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from rich>=13->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.1.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from uvicorn>=0.23.1->mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (8.1.7)\n",
      "Downloading pydantic_ai-0.4.2-py3-none-any.whl (10 kB)\n",
      "Downloading pydantic_ai_slim-0.4.2-py3-none-any.whl (218 kB)\n",
      "Downloading pydantic_evals-0.4.2-py3-none-any.whl (52 kB)\n",
      "Downloading pydantic_graph-0.4.2-py3-none-any.whl (27 kB)\n",
      "Downloading anthropic-0.57.1-py3-none-any.whl (292 kB)\n",
      "Downloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
      "Downloading boto3-1.39.6-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.39.6-py3-none-any.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
      "Downloading cohere-5.16.1-py3-none-any.whl (291 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading fastavro-1.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading google_genai-1.25.0-py3-none-any.whl (226 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "Downloading groq-0.30.0-py3-none-any.whl (131 kB)\n",
      "Downloading logfire_api-3.24.2-py3-none-any.whl (85 kB)\n",
      "Downloading mcp-1.11.0-py3-none-any.whl (155 kB)\n",
      "Downloading mistralai-1.9.2-py3-none-any.whl (411 kB)\n",
      "Downloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sse_starlette-2.4.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: websockets, types-requests, tenacity, rsa, pyyaml, logfire-api, httpx-sse, griffe, fastavro, eval-type-backport, argcomplete, sse-starlette, opentelemetry-api, google-auth, botocore, s3transfer, pydantic-graph, mistralai, groq, google-genai, anthropic, pydantic-ai-slim, mcp, cohere, boto3, pydantic-evals, pydantic-ai\n",
      "\u001b[2K  Attempting uninstall: tenacity\n",
      "\u001b[2K    Found existing installation: tenacity 9.1.2\n",
      "\u001b[2K    Uninstalling tenacity-9.1.2:\n",
      "\u001b[2K      Successfully uninstalled tenacity-9.1.2\n",
      "\u001b[2K  Attempting uninstall: pyyaml\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.1\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.1:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/27\u001b[0m [pyyaml]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/27\u001b[0m [pyyaml]\n",
      "\u001b[2K  Attempting uninstall: botocore[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/27\u001b[0m [google-auth]\n",
      "\u001b[2K    Found existing installation: botocore 1.34.69━━━━━━━━━━━━━\u001b[0m \u001b[32m13/27\u001b[0m [google-auth]\n",
      "\u001b[2K    Uninstalling botocore-1.34.69:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/27\u001b[0m [google-auth]\n",
      "\u001b[2K      Successfully uninstalled botocore-1.34.69━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/27\u001b[0m [botocore]\n",
      "\u001b[2K  Attempting uninstall: s3transfer91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/27\u001b[0m [botocore]\n",
      "\u001b[2K    Found existing installation: s3transfer 0.10.4━━━━━━━━━━━━\u001b[0m \u001b[32m14/27\u001b[0m [botocore]\n",
      "\u001b[2K    Uninstalling s3transfer-0.10.4:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/27\u001b[0m [botocore]\n",
      "\u001b[2K      Successfully uninstalled s3transfer-0.10.4━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/27\u001b[0m [botocore]\n",
      "\u001b[2K  Attempting uninstall: boto3━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m23/27\u001b[0m [cohere]c-ai-slim]\n",
      "\u001b[2K    Found existing installation: boto3 1.34.69m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m23/27\u001b[0m [cohere]\n",
      "\u001b[2K    Uninstalling boto3-1.34.69:━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m23/27\u001b[0m [cohere]\n",
      "\u001b[2K      Successfully uninstalled boto3-1.34.6990m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m23/27\u001b[0m [cohere]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/27\u001b[0m [pydantic-ai]\u001b[0m [boto3]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.39.6 which is incompatible.\n",
      "streamlit 1.32.0 requires rich<14,>=10.14.0, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anthropic-0.57.1 argcomplete-3.6.2 boto3-1.39.6 botocore-1.39.6 cohere-5.16.1 eval-type-backport-0.2.2 fastavro-1.11.1 google-auth-2.40.3 google-genai-1.25.0 griffe-1.7.3 groq-0.30.0 httpx-sse-0.4.0 logfire-api-3.24.2 mcp-1.11.0 mistralai-1.9.2 opentelemetry-api-1.35.0 pydantic-ai-0.4.2 pydantic-ai-slim-0.4.2 pydantic-evals-0.4.2 pydantic-graph-0.4.2 pyyaml-6.0.2 rsa-4.9.1 s3transfer-0.13.0 sse-starlette-2.4.1 tenacity-8.5.0 types-requests-2.32.4.20250611 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydantic-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cfce3d4c-0277-429d-9503-11e2e0957434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent, RunContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3e4cf830-5e34-453f-a1f5-90f60d33ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_agent = Agent(  \n",
    "    'openai:gpt-4o-mini',\n",
    "    system_prompt=developer_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a87a8389-3e58-4cd7-8e3e-e261d93db68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "235631e7-2f63-41bc-8a75-443dc33ce471",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chat_agent.tool\n",
    "def search_tool(ctx: RunContext, query: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Search the FAQ for relevant entries matching the query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : str\n",
    "        The search query string provided by the user.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of search results (up to 5), each containing relevance information \n",
    "        and associated output IDs.\n",
    "    \"\"\"\n",
    "    print(f\"search('{query}')\")\n",
    "    return search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3d6f79e4-26df-46cc-9fb1-a93785e80ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chat_agent.tool\n",
    "def add_entry_tool(ctx: RunContext, question: str, answer: str) -> None:\n",
    "    \"\"\"\n",
    "    Add a new question-answer entry to FAQ.\n",
    "\n",
    "    This function creates a document with the given question and answer, \n",
    "    tagging it as user-added content.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "        The question text to be added to the index.\n",
    "\n",
    "    answer : str\n",
    "        The answer or explanation corresponding to the question.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    return add_entry(question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8c9e5fdb-c087-4566-824b-6d9777f20e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search('course enrollment')\n",
      "Yes, you can join the course! However, you will need to register before the course starts. The course begins on January 15, 2024, at 17:00. Make sure to sign up using the registration link that is provided by the course.\n",
      "\n",
      "Would you like to know how to register or any other details about the course?\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"I just discovered the course. Can I join now?\"\n",
    "agent_run = await chat_agent.run(user_prompt)\n",
    "print(agent_run.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
