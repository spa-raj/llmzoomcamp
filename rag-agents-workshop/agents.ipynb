{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c16169-249c-48fb-b2db-abde63ee0f3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:33:21.614419Z",
     "start_time": "2025-07-16T06:33:21.609160Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from minsearch import AppendableIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ccc1a0-d2be-4783-810f-215dd9c7a866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:34:05.636022Z",
     "start_time": "2025-07-16T06:34:05.107448Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "483e5565-8d81-4897-bc3a-50dd1c5b871d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:34:07.517094Z",
     "start_time": "2025-07-16T06:34:07.113184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x7eff19dd1d00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb005100-6417-4e6e-9d36-eb3b55416522",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:34:10.712507Z",
     "start_time": "2025-07-16T06:34:10.709564Z"
    }
   },
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f5e6af693f61e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n"
     ]
    }
   ],
   "source": [
    "results = search('I just discovered the course. Can I join now?')\n",
    "print(results[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1935842125eaf368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e3eaf8502de1116",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d392d86276677af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "817a30ea-5855-4143-9b1a-258b72e16550",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T13:07:06.017398Z",
     "start_time": "2025-07-15T13:07:03.081001Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'max_iterations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rag(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI just discovered the course. Can I join now?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mrag\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrag\u001b[39m(query):\n\u001b[1;32m     10\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m search(query)\n\u001b[0;32m---> 11\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m build_prompt(query, search_results)\n\u001b[1;32m     12\u001b[0m     answer \u001b[38;5;241m=\u001b[39m llm(prompt)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36mbuild_prompt\u001b[0;34m(query, search_results)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[1;32m     18\u001b[0m     context \u001b[38;5;241m=\u001b[39m context \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msection\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mquestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124manswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(question\u001b[38;5;241m=\u001b[39mquery, context\u001b[38;5;241m=\u001b[39mcontext)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n",
      "\u001b[0;31mKeyError\u001b[0m: 'max_iterations'"
     ]
    }
   ],
   "source": [
    "rag('I just discovered the course. Can I join now?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c521c75-ccfd-46ee-938b-9dc742ead384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T13:07:10.593755Z",
     "start_time": "2025-07-15T13:07:08.523915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The provided context does not include specific instructions on how to run Docker on Gentoo. Therefore, I cannot provide a direct answer to your question about running Docker on that operating system. If you need guidance on this topic, please consult Gentoo's official documentation or Docker's installation guide tailored for Gentoo.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('how do I run docker on gentoo?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6147a0f575953271",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "At the beginning the context is EMPTY.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "If CONTEXT is EMPTY, you can use our FAQ database.\n",
    "In this case, use the following output template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\"\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b37ffc161217da21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "At the beginning the context is EMPTY.\n",
      "\n",
      "<QUESTION>\n",
      "how do I run docker on gentoo?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT> \n",
      "EMPTY\n",
      "</CONTEXT>\n",
      "\n",
      "If CONTEXT is EMPTY, you can use our FAQ database.\n",
      "In this case, use the following output template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\"\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I run docker on gentoo?\"\n",
    "context = \"EMPTY\"\n",
    "\n",
    "prompt = prompt_template.format(question=question, context=context)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4149b195-908a-4b6e-bdd8-658be48b8d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"To run Docker on Gentoo, you need to first install Docker. Here are the general steps: \\n\\n1. **Install Docker**: You can install Docker from the Portage tree. Use the following command: \\n   ```bash\\n   emerge app-emulation/docker\\n   ```\\n   Make sure to enable the necessary USE flags as per your requirement.\\n\\n2. **Add your user to the Docker group**: This allows you to run Docker commands without `sudo`. Use the following command to add your user: \\n   ```bash\\n   usermod -aG docker yourusername\\n   ```\\n   Remember to replace `yourusername` with your actual username.\\n\\n3. **Start the Docker service**: Enable Docker to start automatically at boot with the following command: \\n   ```bash\\n   rc-update add docker default\\n   ```\\n   Then start the Docker service: \\n   ```bash\\n   /etc/init.d/docker start\\n   ```\\n\\n4. **Verify the installation**: You can confirm that Docker is running by executing: \\n   ```bash\\n   docker ps\\n   ```\\n   This should return an empty list, indicating that Docker is correctly set up.\\n\\nAfter these steps, you should be able to run Docker on your Gentoo system.\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "answer = llm(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db6574e6-aded-43bd-9836-2e3d834c4341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"The context is empty, so I cannot find the answer within it. I will need to refer to the FAQ database to find information about how to join the course.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I join the course?\"\n",
    "context = \"EMPTY\"\n",
    "\n",
    "prompt = prompt_template.format(question=question, context=context)\n",
    "answer = llm(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5694d89b-eef9-4fe2-b7fe-93cf0fa4b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "\n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de0d0077-1a49-4670-ad2d-790b76a826ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "At the beginning the context is EMPTY.\n",
      "\n",
      "<QUESTION>\n",
      "how do I join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - Can I still join the course after the start date?\n",
      "answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "</CONTEXT>\n",
      "\n",
      "If CONTEXT is EMPTY, you can use our FAQ database.\n",
      "In this case, use the following output template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\"\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "search_results = search(question)\n",
    "context = build_context(search_results)\n",
    "prompt = prompt_template.format(question=question, context=context)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1a9d8e6-ba16-405d-b879-028680a545aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"To join the course, you need to register before it starts. You can find the registration link in the course information, and be sure to also join the course public Google Calendar for updates. Additionally, make sure to join the course Telegram channel and register on DataTalks.Club's Slack to stay informed about announcements and course-related communication.\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "answer = llm(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30fa9499-9e9e-4630-8926-1d515b6473bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_rag_v1(question):\n",
    "    context = \"EMPTY\"\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(answer)\n",
    "\n",
    "    if answer['action'] == 'SEARCH':\n",
    "        print('need to perform search...')\n",
    "        search_results = search(question)\n",
    "        context = build_context(search_results)\n",
    "        \n",
    "        prompt = prompt_template.format(question=question, context=context)\n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(answer)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0181a69-51db-4c82-9bd7-263a075bb7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'ANSWER', 'answer': \"To join the course, you typically need to visit the course's official website or the platform where it is hosted, register for an account if you don't have one, and then follow the registration process for the specific course you are interested in. This may involve paying a fee or submitting an application, depending on the course requirements.\", 'source': 'OWN_KNOWLEDGE'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To join the course, you typically need to visit the course's official website or the platform where it is hosted, register for an account if you don't have one, and then follow the registration process for the specific course you are interested in. This may involve paying a fee or submitting an application, depending on the course requirements.\",\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_rag_v1('how do I join the course?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f0e02cf-1088-46c2-b788-d27940d1617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'ANSWER', 'answer': \"To patch KDE under FreeBSD, you typically follow these steps: \\n\\n1. **Install the necessary tools**: Make sure you have 'git' and 'patch' installed in your FreeBSD system. You can install them using the package manager:\\n   ```bash\\n   pkg install git patch\\n   ```\\n\\n2. **Get the source code**: If you haven't already, you need to obtain the KDE source code. You can find it in the FreeBSD ports collection, or you can clone it directly from the KDE repositories:\\n   ```bash\\n   git clone https://invent.kde.org/packaging/kde-freebsd.git\\n   ```\\n\\n3. **Download the patch**: Obtain the patch file that you would like to apply to the source code. This could be from the KDE project website or a bug tracker.\\n\\n4. **Apply the patch**: Navigate to the directory where you have the KDE sources and apply the patch using the following command:\\n   ```bash\\n   patch -p1 < /path/to/your/patch/file.patch\\n   ```\\n   Ensure that you are using the correct path to your patch file.\\n\\n5. **Build and install the patched version**: After applying the patch, you'll need to compile the software. This usually involves:\\n   ```bash\\n   cd /path/to/kde/source\\n   make install\\n   ```\\n\\n6. **Test the installation**: Finally, run the patched KDE application to ensure that the patch has been applied correctly and that everything is functioning as expected.\\n\\nRemember to check the documentation for the specific version of KDE you are working with, as the steps may differ slightly depending on the complexity of the patch or the specific version of FreeBSD being used.\", 'source': 'OWN_KNOWLEDGE'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To patch KDE under FreeBSD, you typically follow these steps: \\n\\n1. **Install the necessary tools**: Make sure you have 'git' and 'patch' installed in your FreeBSD system. You can install them using the package manager:\\n   ```bash\\n   pkg install git patch\\n   ```\\n\\n2. **Get the source code**: If you haven't already, you need to obtain the KDE source code. You can find it in the FreeBSD ports collection, or you can clone it directly from the KDE repositories:\\n   ```bash\\n   git clone https://invent.kde.org/packaging/kde-freebsd.git\\n   ```\\n\\n3. **Download the patch**: Obtain the patch file that you would like to apply to the source code. This could be from the KDE project website or a bug tracker.\\n\\n4. **Apply the patch**: Navigate to the directory where you have the KDE sources and apply the patch using the following command:\\n   ```bash\\n   patch -p1 < /path/to/your/patch/file.patch\\n   ```\\n   Ensure that you are using the correct path to your patch file.\\n\\n5. **Build and install the patched version**: After applying the patch, you'll need to compile the software. This usually involves:\\n   ```bash\\n   cd /path/to/kde/source\\n   make install\\n   ```\\n\\n6. **Test the installation**: Finally, run the patched KDE application to ensure that the patch has been applied correctly and that everything is functioning as expected.\\n\\nRemember to check the documentation for the specific version of KDE you are working with, as the steps may differ slightly depending on the complexity of the patch or the specific version of FreeBSD being used.\",\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_rag_v1('how patch KDE under FreeBSD?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be6bcf-e7ff-492c-8f62-3025742bce5f",
   "metadata": {},
   "source": [
    "## Agentic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87973665-b4f3-4a96-b3a5-8bb0edb57a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "\n",
    "The CONTEXT is build with the documents from our FAQ database.\n",
    "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
    "from FAQ to and add them to the context.\n",
    "PREVIOUS_ACTIONS contains the actions you already performed.\n",
    "\n",
    "At the beginning the CONTEXT is empty.\n",
    "\n",
    "You can perform the following actions:\n",
    "\n",
    "- Search in the FAQ database to get more data for the CONTEXT\n",
    "- Answer the question using the CONTEXT\n",
    "- Answer the question using your own knowledge\n",
    "\n",
    "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
    "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
    "\n",
    "Don't use search queries used at the previous iterations.\n",
    "\n",
    "\n",
    "Don't repeat previously performed actions.\n",
    "\n",
    "Don't perform more than {max_iterations} iterations for a given student question.\n",
    "The current iteration number: {iteration_number}. If we exceed the allowed number \n",
    "of iterations, give the best possible answer with the provided information.\n",
    "\n",
    "\n",
    "Output templates:\n",
    "\n",
    "If you want to perform search, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\",\n",
    "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER_CONTEXT\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<SEARCH_QUERIES>\n",
    "{search_queries}\n",
    "</SEARCH_QUERIES>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "<PREVIOUS_ACTIONS>\n",
    "{previous_actions}\n",
    "</PREVIOUS_ACTIONS>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e7f39f7-aeb6-42fa-b010-0b366f19967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "how do I join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I join the course?\"\n",
    "\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []\n",
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=3,\n",
    "    iteration_number=1\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29f5989b-949f-478a-abea-048b1bcfa1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)\n",
    "answer = json.loads(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95372972-6767-4c98-ba68-57a45326918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_actions.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6ee2191-3035-427c-83f8-a6516ec6ea53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'SEARCH',\n",
       "  'reasoning': \"The student is asking about how to join the course, which likely involves enrollment procedures or requirements that haven't been provided in the current context. More information from the FAQ database could clarify the process.\",\n",
       "  'keywords': ['how to join the course',\n",
       "   'enrollment process',\n",
       "   'course registration instructions']}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4097b3d-2d46-4ba0-9444-da92d604cd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"The student is asking about how to join the course, which likely involves enrollment procedures or requirements that haven't been provided in the current context. More information from the FAQ database could clarify the process.\",\n",
      "  \"keywords\": [\n",
      "    \"how to join the course\",\n",
      "    \"enrollment process\",\n",
      "    \"course registration instructions\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(answer, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9d091f8-8765-4012-971e-ad4498cab7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = answer['keywords']\n",
    "search_queries.extend(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08d93d61-5762-435d-a388-19c71010b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in keywords:\n",
    "    res = search(k)\n",
    "    search_results.extend(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd6e476c-fef7-4b57-9fdc-79e3a8587de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f35dfffd-82ec-492c-acf1-e7245a80578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(seq):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for el in seq:\n",
    "        _id = el['_id']\n",
    "        if _id in seen:\n",
    "            continue\n",
    "        seen.add(_id)\n",
    "        result.append(el)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ddb8a453-60a4-436d-a4db-10191cf3c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = dedup(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae4336eb-7d37-4037-956e-2e21bea39f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3eaa30b-edc7-449a-a6cd-b80f6081b862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "how do I join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "how to join the course\n",
      "enrollment process\n",
      "course registration instructions\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - Can I still join the course after the start date?\n",
      "answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: lsRuntimeError: Java gateway process exited before sending its port number\n",
      "answer: After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\n",
      "import pyspark\n",
      "from pyspark.sql import SparkSession\n",
      "spark = SparkSession.builder \\\n",
      ".master(\"local[*]\") \\\n",
      ".appName('test') \\\n",
      ".getOrCreate()\n",
      "df = spark.read \\\n",
      ".option(\"header\", \"true\") \\\n",
      ".csv('taxi+_zone_lookup.csv')\n",
      "df.show()\n",
      "it gives the error:\n",
      "RuntimeError: Java gateway process exited before sending its port number\n",
      "✅The solution (for me) was:\n",
      "pip install findspark on the command line and then\n",
      "Add\n",
      "import findspark\n",
      "findspark.init()\n",
      "to the top of the script.\n",
      "Another possible solution is:\n",
      "Check that pyspark is pointing to the correct location.\n",
      "Run pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\n",
      "If it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\n",
      "To add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\n",
      "Once everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.\n",
      "\n",
      "section: Module 2: Workflow Orchestration\n",
      "question: Process to download the VSC using Pandas is killed right away\n",
      "answer: pd.read_csv\n",
      "df_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)\n",
      "The data needs to be appended to the parquet file using the fastparquet engine\n",
      "df.to_parquet(path, compression=\"gzip\", engine='fastparquet', append=True)\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker-Compose - Errors pertaining to docker-compose.yml and pgadmin setup\n",
      "answer: For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\n",
      "create a new volume on docker (either using the command line or docker desktop app)\n",
      "make the following changes to your docker-compose.yml file (see attachment)\n",
      "set low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\n",
      "use the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\n",
      "Order of execution:\n",
      "(1) open terminal in 2_docker_sql folder and run docker compose up\n",
      "(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\n",
      "(3) open jupyter notebook and begin the data ingestion\n",
      "(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.\n",
      "\n",
      "section: Module 2: Workflow Orchestration\n",
      "question: GCP - 2.2.7d Part 2 - Getting error when you run terraform apply\n",
      "answer: If you get the following error\n",
      "You have to edit variables.tf on the gcp folder, set your project-id and region and zones properly. Then, run terraform apply again.\n",
      "You can find correct regions/zones here: https://cloud.google.com/compute/docs/regions-zones\n",
      "Deploying MAGE to GCP  with Terraform via the VM (2.2.7)\n",
      "FYI - It can take up to 20 minutes to deploy the MAGE Terraform files if you are using a GCP Virtual Machine. It is normal, so don’t interrupt the process or think it’s taking too long. If you have, make sure you run a terraform destroy before trying again as you will have likely partially created resources which will cause errors next time you run `terraform apply`.\n",
      "`terraform destroy` may not completely delete partial resources - go to Google Cloud Console and use the search bar at the top to search for the ‘app.name’ you declared in your variables.tf file; this will list all resources with that name - make sure you delete them all before running `terraform apply` again.\n",
      "Why are my GCP free credits going so fast? MAGE .tf files - Terraform Destroy not destroying all Resources\n",
      "I checked my GCP billing last night & the MAGE Terraform IaC didn't destroy a GCP Resource called Filestore as ‘mage-data-prep- it has been costing £5.01 of my free credits each day  I now have £151 left - Alexey has assured me that This amount WILL BE SUFFICIENT funds to finish the course. Note to anyone who had issues deploying the MAGE terraform code: check your billing account to see what you're being charged for (main menu - billing) (even if it's your free credits) and run a search for 'mage-data-prep' in the top bar just to be sure that your resources have been destroyed - if any come up delete them.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\n",
      "answer: You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"The student is asking about how to join the course, which likely involves enrollment procedures or requirements that haven't been provided in the current context. More information from the FAQ database could clarify the process.\", \"keywords\": [\"how to join the course\", \"enrollment process\", \"course registration instructions\"]}\n",
      "</PREVIOUS_ACTIONS>\n"
     ]
    }
   ],
   "source": [
    "# question = \"how do I join the course?\"\n",
    "\n",
    "# search_queries = []\n",
    "# search_results = []\n",
    "# previous_actions = []\n",
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=3,\n",
    "    iteration_number=2\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff78c687-a2c2-439b-a0af-19fbc09d94b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"action\": \"ANSWER_CONTEXT\",\n",
      "  \"answer\": \"To join the course, you need to register before the course starts using the provided link. The course will begin on January 15, 2024, at 17:00. You also have the option to join the course's Telegram channel for announcements and are encouraged to register in DataTalks.Club's Slack and join the relevant channel. Even if you don't register, you can still submit homework, but there are deadlines for the final projects, so it's best to manage your time wisely.\",\n",
      "  \"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "answer_json = llm(prompt)\n",
    "answer = json.loads(answer_json)\n",
    "print(json.dumps(answer, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "652db0b9-fbdd-4df5-b973-ac6cc0711213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"The question is about success in module 1, which may involve specific strategies, resources, or guidelines that could be included in the FAQ database. I need more information on what is required for success in that module.\",\n",
      "  \"keywords\": [\n",
      "    \"success in module 1\",\n",
      "    \"strategies for module 1\",\n",
      "    \"requirements for passing module 1\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "requirements for passing module 1\n",
      "success in module 1\n",
      "strategies for module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e…\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\n",
      "answer: Error raised during the jupyter notebook’s cell execution:\n",
      "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\n",
      "Solution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Module “kafka” not found when trying to run producer.py\n",
      "answer: Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\n",
      "To create a virtual env and install packages (run only once)\n",
      "python -m venv env\n",
      "source env/bin/activate\n",
      "pip install -r ../requirements.txt\n",
      "To activate it (you'll need to run it every time you need the virtual env):\n",
      "source env/bin/activate\n",
      "To deactivate it:\n",
      "deactivate\n",
      "This works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\n",
      "Also the virtual environment should be created only to run the python file. Docker images should first all be up and running.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"The question is about success in module 1, which may involve specific strategies, resources, or guidelines that could be included in the FAQ database. I need more information on what is required for success in that module.\", \"keywords\": [\"success in module 1\", \"strategies for module 1\", \"requirements for passing module 1\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"To provide a comprehensive answer about what is needed to be successful in Module 1, I should look for specific requirements, strategies, and tips for success in that module within the FAQ database.\",\n",
      "  \"keywords\": [\n",
      "    \"requirements for passing module 1\",\n",
      "    \"success strategies for Module 1\",\n",
      "    \"tips for success in Docker and Terraform\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #2...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "requirements for passing module 1\n",
      "success in module 1\n",
      "strategies for module 1\n",
      "tips for success in Docker and Terraform\n",
      "success strategies for Module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e…\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\n",
      "answer: Error raised during the jupyter notebook’s cell execution:\n",
      "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\n",
      "Solution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Module “kafka” not found when trying to run producer.py\n",
      "answer: Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\n",
      "To create a virtual env and install packages (run only once)\n",
      "python -m venv env\n",
      "source env/bin/activate\n",
      "pip install -r ../requirements.txt\n",
      "To activate it (you'll need to run it every time you need the virtual env):\n",
      "source env/bin/activate\n",
      "To deactivate it:\n",
      "deactivate\n",
      "This works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\n",
      "Also the virtual environment should be created only to run the python file. Docker images should first all be up and running.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Error: Failed to query available provider packages │ Could not retrieve the list of available versions for provider hashicorp/google: could not query │ provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts, │ please try again later\n",
      "answer: It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Error:Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout\n",
      "answer: The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Install for WSL\n",
      "answer: https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Error acquiring the state lock\n",
      "answer: https://github.com/hashicorp/terraform/issues/14513\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Error 400 Bad Request.  Invalid JWT Token  on WSL.\n",
      "answer: When running\n",
      "terraform apply\n",
      "on wsl2 I've got this error:\n",
      "│ Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\n",
      "│ Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\n",
      "IT happens because there may be time desync on your machine which affects computing JWT\n",
      "To fix this, run the command\n",
      "sudo hwclock -s\n",
      "which fixes your system time.\n",
      "Reference\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"The question is about success in module 1, which may involve specific strategies, resources, or guidelines that could be included in the FAQ database. I need more information on what is required for success in that module.\", \"keywords\": [\"success in module 1\", \"strategies for module 1\", \"requirements for passing module 1\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a comprehensive answer about what is needed to be successful in Module 1, I should look for specific requirements, strategies, and tips for success in that module within the FAQ database.\", \"keywords\": [\"requirements for passing module 1\", \"success strategies for Module 1\", \"tips for success in Docker and Terraform\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER\",\n",
      "  \"answer\": \"To be successful in Module 1, which focuses on Docker and Terraform, it is essential to understand the core concepts of these technologies, work on practical exercises, and be familiar with troubleshooting common issues. Here are some key steps you can take:\\n\\n1. **Study the Learning Material**: Make sure to go through the provided learning resources and documentation for Docker and Terraform thoroughly.\\n\\n2. **Hands-on Practice**: Install Docker and Terraform on your machine, and try out practical exercises to solidify your understanding.\\n\\n3. **Resolve Installation Issues**: Be prepared to troubleshoot installation or module errors. For example, common errors like 'ModuleNotFoundError' for certain Python packages should prompt you to install those packages correctly using pip or conda.\\n\\n4. **Check Network Settings**: If you encounter network-related issues (like failing to access provider packages), ensure that your VPN settings or firewall are not blocking the required connections.\\n\\n5. **Engage with the Community**: Don't hesitate to ask questions or seek help from peers or support resources if you run into challenges. Collaboration can enhance your learning experience.\\n\\n6. **Regularly Test Your Knowledge**: Utilize quizzes and practice tests, if available, to assess your understanding of the materials. \\n\\nBy staying committed and proactive in your learning, you'll greatly increase your chances of success in this module.\",\n",
      "  \"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"what do I need to do to be successful at module 1?\"\n",
    "\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []\n",
    "\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    print(f'ITERATION #{iteration}...')\n",
    "\n",
    "    context = build_context(search_results)\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        search_queries=\"\\n\".join(search_queries),\n",
    "        previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "        max_iterations=3,\n",
    "        iteration_number=iteration\n",
    "    )\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(json.dumps(answer, indent=2))\n",
    "\n",
    "    previous_actions.append(answer)\n",
    "\n",
    "    action = answer['action']\n",
    "    if action != 'SEARCH':\n",
    "        break\n",
    "\n",
    "    keywords = answer['keywords']\n",
    "    search_queries = list(set(search_queries) | set(keywords))\n",
    "    \n",
    "    for k in keywords:\n",
    "        res = search(k)\n",
    "        search_results.extend(res)\n",
    "\n",
    "    search_results = dedup(search_results)\n",
    "    \n",
    "    iteration = iteration + 1\n",
    "    if iteration >= 4:\n",
    "        break\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2ee162d-15ec-4636-8b6b-8a1265d72ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_search(question):\n",
    "    search_queries = []\n",
    "    search_results = []\n",
    "    previous_actions = []\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        print(f'ITERATION #{iteration}...')\n",
    "    \n",
    "        context = build_context(search_results)\n",
    "        prompt = prompt_template.format(\n",
    "            question=question,\n",
    "            context=context,\n",
    "            search_queries=\"\\n\".join(search_queries),\n",
    "            previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "            max_iterations=3,\n",
    "            iteration_number=iteration\n",
    "        )\n",
    "    \n",
    "        print(prompt)\n",
    "    \n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(json.dumps(answer, indent=2))\n",
    "\n",
    "        previous_actions.append(answer)\n",
    "    \n",
    "        action = answer['action']\n",
    "        if action != 'SEARCH':\n",
    "            break\n",
    "    \n",
    "        keywords = answer['keywords']\n",
    "        search_queries = list(set(search_queries) | set(keywords))\n",
    "\n",
    "        for k in keywords:\n",
    "            res = search(k)\n",
    "            search_results.extend(res)\n",
    "    \n",
    "        search_results = dedup(search_results)\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "        if iteration >= 4:\n",
    "            break\n",
    "    \n",
    "        print()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7de2df44-d9fd-4b9c-baa1-ef462c4d8701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"I need to gather relevant information about course preparation from the FAQ database, as the current context is empty.\",\n",
      "  \"keywords\": [\n",
      "    \"course preparation\",\n",
      "    \"how to prepare for the course\",\n",
      "    \"course requirements\",\n",
      "    \"study tips\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "course preparation\n",
      "study tips\n",
      "how to prepare for the course\n",
      "course requirements\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - What can I do before the course starts?\n",
      "answer: You can start by installing and setting up all the dependencies and requirements:\n",
      "Google cloud account\n",
      "Google Cloud SDK\n",
      "Python 3 (installed with Anaconda)\n",
      "Terraform\n",
      "Git\n",
      "Look over the prerequisites and syllabus to see if you are comfortable with these subjects.\n",
      "\n",
      "section: Module 3: Data Warehousing\n",
      "question: GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?\n",
      "answer: Check the Schema\n",
      "You might have a wrong formatting\n",
      "Try to upload the CSV.GZ files without formatting or going through pandas via wget\n",
      "See this Slack conversation for helpful tips\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n",
      "answer: If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\n",
      "For Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\bin” to the PATH variable.\n",
      "Additional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"I need to gather relevant information about course preparation from the FAQ database, as the current context is empty.\", \"keywords\": [\"course preparation\", \"how to prepare for the course\", \"course requirements\", \"study tips\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER_CONTEXT\",\n",
      "  \"answer\": \"Before the course starts, you can prepare by installing and setting up all the necessary dependencies and requirements, which include a Google Cloud account, Google Cloud SDK, Python 3 (with Anaconda), Terraform, and Git. Additionally, review the prerequisites and the syllabus to ensure you are comfortable with the subjects that will be covered in the course.\",\n",
      "  \"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER_CONTEXT',\n",
       " 'answer': 'Before the course starts, you can prepare by installing and setting up all the necessary dependencies and requirements, which include a Google Cloud account, Google Cloud SDK, Python 3 (with Anaconda), Terraform, and Git. Additionally, review the prerequisites and the syllabus to ensure you are comfortable with the subjects that will be covered in the course.',\n",
       " 'source': 'CONTEXT'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_search('how do I prepare for the course?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66788038-148f-4da1-adfe-22de6e1773c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T07:18:49.782329Z",
     "start_time": "2025-07-16T07:18:49.779952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'ANSWER', 'answer': \"To be successful in Module 1, which focuses on Docker and Terraform, it is essential to understand the core concepts of these technologies, work on practical exercises, and be familiar with troubleshooting common issues. Here are some key steps you can take:\\n\\n1. **Study the Learning Material**: Make sure to go through the provided learning resources and documentation for Docker and Terraform thoroughly.\\n\\n2. **Hands-on Practice**: Install Docker and Terraform on your machine, and try out practical exercises to solidify your understanding.\\n\\n3. **Resolve Installation Issues**: Be prepared to troubleshoot installation or module errors. For example, common errors like 'ModuleNotFoundError' for certain Python packages should prompt you to install those packages correctly using pip or conda.\\n\\n4. **Check Network Settings**: If you encounter network-related issues (like failing to access provider packages), ensure that your VPN settings or firewall are not blocking the required connections.\\n\\n5. **Engage with the Community**: Don't hesitate to ask questions or seek help from peers or support resources if you run into challenges. Collaboration can enhance your learning experience.\\n\\n6. **Regularly Test Your Knowledge**: Utilize quizzes and practice tests, if available, to assess your understanding of the materials. \\n\\nBy staying committed and proactive in your learning, you'll greatly increase your chances of success in this module.\", 'source': 'OWN_KNOWLEDGE'}\n"
     ]
    }
   ],
   "source": [
    "print(globals()['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353d1d5-d7c9-4f27-b12e-3dc51d27be85",
   "metadata": {},
   "source": [
    "## Tools (function calling)\n",
    "\n",
    "https://platform.openai.com/docs/guides/function-calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e854ad2f-4bfa-48fa-bab1-f925afbdc0cc",
   "metadata": {},
   "source": [
    "    def search(query):\n",
    "        boost = {'question': 3.0, 'section': 0.5}\n",
    "    \n",
    "        results = index.search(\n",
    "            query=query,\n",
    "            filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "            boost_dict=boost,\n",
    "            num_results=5,\n",
    "            output_ids=True\n",
    "        )\n",
    "    \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28133938-0af4-4e9d-825f-4f44d42b5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df28b463-990a-482d-ba49-c55403bbf2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0ac6e7d-bbc1-40fd-8b34-249f9805f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I do well in module 1?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c48fb94b-5a14-4d7c-a695-f9672c846ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "81502be5-0e65-4364-8c64-9dee771f5a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_6878420eab6c81a38fa77d4a7efa902601a79e85977e5feb', created_at=1752711694.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFunctionToolCall(arguments='{\"query\":\"module 1 tips\"}', call_id='call_OhxNXDKj9HBqqrlGpoWapXuk', name='search', type='function_call', id='fc_6878420f5c8081a38daeafd26e17c4ce01a79e85977e5feb', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='search', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'Search query text to look up in the course FAQ.'}}, 'required': ['query'], 'additionalProperties': False}, strict=True, type='function', description='Search the FAQ database')], top_p=1.0, background=False, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=84, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=17, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=101), user=None, store=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "781242a3-c3b0-4083-9adc-d88aa8797885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"query\":\"module 1 tips\"}', call_id='call_OhxNXDKj9HBqqrlGpoWapXuk', name='search', type='function_call', id='fc_6878420f5c8081a38daeafd26e17c4ce01a79e85977e5feb', status='completed')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8c5723a-cbf4-4e71-a07b-73ca4016c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.choices[0].message.content\n",
    "calls = response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7759e3c4-835a-4fee-9f15-30b19706ace8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseFunctionToolCall(arguments='{\"query\":\"module 1 tips\"}', call_id='call_OhxNXDKj9HBqqrlGpoWapXuk', name='search', type='function_call', id='fc_6878420f5c8081a38daeafd26e17c4ce01a79e85977e5feb', status='completed')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call = calls[0]\n",
    "call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e14a75d1-981e-44f7-bc2b-bf24509d11f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call_OhxNXDKj9HBqqrlGpoWapXuk'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call.call_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4226a3ff-4cd0-4e2f-9c22-96c7a035ba4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_name = call.name\n",
    "f_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c6fb3740-089f-4c26-a3bc-87dbc9e145c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'module 1 tips'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments = json.loads(call.arguments)\n",
    "arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "11d47c39-2688-432a-afc8-b7d481487123",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = locals()[f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac44a26e-f5b3-4e7f-997a-ba709d0bef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = f(**arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81342abc-e9f3-4fa5-a546-fd317c292b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
      "    \"section\": \"Module 4: analytics engineering with dbt\",\n",
      "    \"question\": \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 299\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Issue:\\ne\\u2026\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the \\u201c ModuleNotFoundError: No module named 'psycopg2' \\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
      "    \"section\": \"Module 1: Docker and Terraform\",\n",
      "    \"question\": \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 112\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \\\"TypeError: 'module' object is not callable\\\"\\nSolution:\\nconn_string = \\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\"\\nengine = create_engine(conn_string)\",\n",
      "    \"section\": \"Module 1: Docker and Terraform\",\n",
      "    \"question\": \"Python - SQLALchemy - TypeError 'module' object is not callable\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 124\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Error raised during the jupyter notebook\\u2019s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module \\u201cpsycopg2\\u201d. Can be installed by Conda or pip.\",\n",
      "    \"section\": \"Module 1: Docker and Terraform\",\n",
      "    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 125\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\u201d\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\"` appropriately.\\nAdditionally, you can check for the version of \\u2018py4j\\u2019 of the spark you\\u2019re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\n",
      "    \"section\": \"Module 5: pyspark\",\n",
      "    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"_id\": 323\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "search_results = json.dumps(results, indent=2)\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "572bc1e3-51b0-4dc1-9030-131a9a057c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages.append(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1099da2-9710-4d24-865f-9d59ecfc9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages.append({\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": search_results,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b41e545-626f-471c-b9a7-4d74efc2f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5576052f-7244-4b3c-ac18-3e3d305739dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = response.output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d91eb0af-cf09-410e-93ca-407aebf551b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To do well in Module 1, here are some general tips:\n",
      "\n",
      "1. **Understand Key Concepts**: Familiarize yourself with the core concepts covered in the module. Focus on the basics of Docker and Terraform, as these are essential for the course.\n",
      "\n",
      "2. **Hands-On Practice**: Set up Docker and try running sample applications. The more you practice, the more comfortable you will become with the tools.\n",
      "\n",
      "3. **Troubleshooting**: Pay attention to common errors (like the `ModuleNotFoundError` for `psycopg2`). Make sure you know how to resolve these by installing the necessary packages.\n",
      "\n",
      "4. **Ask Questions**: If you're stuck, don’t hesitate to ask questions in the forums or during live sessions. Engaging with fellow students and instructors can provide clarity.\n",
      "\n",
      "5. **Use Resources Wisely**: Utilize any recommended readings, videos, or documentation provided in the course. They can offer deeper insights into the topics discussed.\n",
      "\n",
      "6. **Stay Organized**: Keep your work organized. Document your learning and issues you encounter, and how you resolved them. This will help if similar issues arise later.\n",
      "\n",
      "7. **Review Regularly**: Go back through the materials regularly to reinforce your learning. Make sure you understand each part before moving on.\n",
      "\n",
      "By following these tips, you should be able to perform well in Module 1. Good luck!\n"
     ]
    }
   ],
   "source": [
    "print(r.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e6195a9d-f737-415e-bfb5-a413d449c94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'message'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ee391d2-8b2a-4b8c-85f7-6a56d5fa3f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'function_call'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call.type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5746ea-4d80-4eee-8d16-95305cc66ebc",
   "metadata": {},
   "source": [
    "### Multiple calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fd69ffd0-2a4b-454e-87bd-2ff976d61a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_call(tool_call_response):\n",
    "    function_name = tool_call_response.name\n",
    "    arguments = json.loads(tool_call_response.arguments)\n",
    "\n",
    "    f = globals()[function_name]\n",
    "    result = f(**arguments)\n",
    "\n",
    "    return {\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": tool_call_response.call_id,\n",
    "        \"output\": json.dumps(result, indent=2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f5567651-5716-4d47-9002-d347bd69b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "If you look up something in FAQ, convert the student question into multiple queries.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e2176d6f-8d0b-4e12-8b2d-64bbf9cc9961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_68784232dca0819c9ea3abc4db0b546d005bae8ac02d6d62', created_at=1752711730.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFunctionToolCall(arguments='{\"query\":\"how to do well in module 1\"}', call_id='call_Wdb0aEYe97vxcqMwL5ScqcYd', name='search', type='function_call', id='fc_687842349574819c974eb8e1a0302d68005bae8ac02d6d62', status='completed'), ResponseFunctionToolCall(arguments='{\"query\":\"module 1 tips for success\"}', call_id='call_Z45OmEaFHdlPbGitqZrhwJMV', name='search', type='function_call', id='fc_68784234fe34819c9d2649972ffcb452005bae8ac02d6d62', status='completed'), ResponseFunctionToolCall(arguments='{\"query\":\"study strategies for module 1\"}', call_id='call_mclv98vemGIG5AhsoGhO4toQ', name='search', type='function_call', id='fc_687842355c0c819ca967ca4f37ffb2e8005bae8ac02d6d62', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='search', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'Search query text to look up in the course FAQ.'}}, 'required': ['query'], 'additionalProperties': False}, strict=True, type='function', description='Search the FAQ database')], top_p=1.0, background=False, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=99, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=72, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=171), user=None, store=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac9a762c-1836-4573-b0e5-c927e2fb47b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call\n",
      "function_call\n",
      "function_call\n"
     ]
    }
   ],
   "source": [
    "for entry in response.output:\n",
    "    chat_messages.append(entry)\n",
    "    print(entry.type)\n",
    "\n",
    "    if entry.type == 'function_call':      \n",
    "        result = do_call(entry)\n",
    "        chat_messages.append(result)\n",
    "    elif entry.type == 'message':\n",
    "        print(entry.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "caee8500-ffc9-4aa8-9f88-283f3a5d9446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message\n",
      "\n",
      "To do well in Module 1, here are some tips and strategies:\n",
      "\n",
      "1. **Understanding Prerequisites**:\n",
      "   - Make sure you have all the necessary tools and libraries installed, such as Docker and PostgreSQL.\n",
      "   - It's essential to install the Python module `psycopg2`. You can do this using:\n",
      "     ```bash\n",
      "     pip install psycopg2-binary\n",
      "     ```\n",
      "\n",
      "2. **Handling Common Errors**:\n",
      "   - If you encounter the error **ModuleNotFoundError: No module named 'psycopg2'**, ensure that you have installed the module correctly or update it:\n",
      "     ```bash\n",
      "     pip install psycopg2-binary --upgrade\n",
      "     ```\n",
      "\n",
      "3. **Follow the Course Content Diligently**:\n",
      "   - Engage with the provided tutorials and resources, and follow the instructions carefully, especially regarding Docker configurations.\n",
      "\n",
      "4. **Practice with Docker**:\n",
      "   - Familiarize yourself with Docker commands like `docker-compose build` and `docker-compose run`. Running these commands correctly is vital for setting up your environment.\n",
      "\n",
      "5. **Experiment and Test**:\n",
      "   - Experiment with different configurations and test your setup regularly to ensure everything works as expected. \n",
      "   - Create sample projects to apply the concepts you learn, such as creating a simple database using Docker.\n",
      "\n",
      "6. **Engagement and Discussion**:\n",
      "   - Engage with fellow students or course forums for discussions. Sharing knowledge and asking questions can significantly enhance your understanding.\n",
      "\n",
      "7. **Utilize Resources**:\n",
      "   - Leverage online resources and documentation on Docker and PostgreSQL when needed.\n",
      "\n",
      "In summary, focus on installing necessary dependencies, follow the course structure closely, engage with the community, practice thoroughly, and don't hesitate to ask for help when needed. Good luck with Module 1!\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "for entry in response.output:\n",
    "    chat_messages.append(entry)\n",
    "    print(entry.type)\n",
    "    print()\n",
    "\n",
    "    if entry.type == 'function_call':      \n",
    "        result = do_call(entry)\n",
    "        chat_messages.append(result)\n",
    "    elif entry.type == 'message':\n",
    "        print(entry.content[0].text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52fc54b-10a1-4aa8-88e5-dab893e612ee",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722923e7-9fe3-4561-bf3f-682de839a6ac",
   "metadata": {},
   "source": [
    "Have two loops:\n",
    "\n",
    "- First is the main Q&A loop - ask question, get back the answer\n",
    "- Second is the request loop - send requests until there's a message reply from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "87be0bfd-7fd8-4102-b05d-f6ef4fd48fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "When using FAQ, perform deep topic exploration: make one request to FAQ,\n",
    "and then based on the results, make more requests.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9615aa6a-4103-4e04-b327-f2009d8214bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " How do I do well in module 4?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"module 4\"}', call_id='call_6sxBYT0oBlPTWPmSX4K3cD6M', name='search', type='function_call', id='fc_6878425a46e8819fae7f78602c1c26b50baeeafc8a3abd7c', status='completed')\n",
      "\n",
      "To do well in **Module 4: Analytics Engineering with dbt**, here are some key tips:\n",
      "\n",
      "1. **Understand dbt Fundamentals**: Make sure you grasp the core concepts of dbt, including models, sources, and tests.\n",
      "\n",
      "2. **Hands-on Practice**: Engage in practical exercises by working through examples provided in the course materials. This will help solidify your understanding.\n",
      "\n",
      "3. **Utilize Docker**: If you're setting up dbt with Docker, ensure your environment is correctly configured. If you run into the error `ModuleNotFoundError: No module named 'pytz'`, you can resolve this by adding `RUN python -m pip install --no-cache pytz` in your Dockerfile.\n",
      "\n",
      "4. **Collaborate and Discuss**: Join your peers in study groups or forums to discuss assignments and clarify doubts. This interaction can reveal different perspectives on the material.\n",
      "\n",
      "5. **Seek Help for Errors**: Don’t hesitate to seek help if you encounter issues, whether they are installation problems or questions regarding dbt functionality.\n",
      "\n",
      "Would you like more specific resources or examples related to any of these points?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " where can I get more hands on experience with dbt?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"hands-on experience with dbt\"}', call_id='call_wjJzOyPgaQhFd9PVRYhkzgzE', name='search', type='function_call', id='fc_687842777b20819fb06011a47646b95c0baeeafc8a3abd7c', status='completed')\n",
      "\n",
      "To gain more hands-on experience with dbt, consider the following options:\n",
      "\n",
      "1. **dbt Cloud**: Leverage dbt Cloud where you can create and manage your dbt projects. This platform provides built-in CI/CD workflows, making it easier to test and deploy models.\n",
      "\n",
      "2. **Local Environment Setup**: Run dbt locally. Follow the documentation to set up dbt in your local environment. Use Docker if you prefer containerization.\n",
      "\n",
      "3. **Project Work**: Engage in personal projects or contribute to open-source dbt projects. This not only helps in solidifying your knowledge, but also allows you to tackle real-world problems.\n",
      "\n",
      "4. **Online Courses and Tutorials**: Look for online courses that feature practical assignments related to dbt. Websites like GitHub may also have repositories dedicated to dbt tutorials.\n",
      "\n",
      "5. **Practice with Existing Datasets**: Utilize platforms that provide public datasets (like BigQuery samples) to practice building models with dbt.\n",
      "\n",
      "6. **Study Groups**: Join study groups or forums focused on analytics engineering and dbt, as discussing challenges with peers can enhance your learning.\n",
      "\n",
      "Is there a specific option among these that interests you more, or do you need further details on any of them?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " stop\n"
     ]
    }
   ],
   "source": [
    "while True: # main Q&A loop\n",
    "    question = input() # How do I do my best for module 1?\n",
    "    if question == 'stop':\n",
    "        break\n",
    "\n",
    "    message = {\"role\": \"user\", \"content\": question}\n",
    "    chat_messages.append(message)\n",
    "\n",
    "    while True: # request-response loop - query API till get a message\n",
    "        response = client.responses.create(\n",
    "            model='gpt-4o-mini',\n",
    "            input=chat_messages,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "        has_messages = False\n",
    "        \n",
    "        for entry in response.output:\n",
    "            chat_messages.append(entry)\n",
    "        \n",
    "            if entry.type == 'function_call':      \n",
    "                print('function_call:', entry)\n",
    "                print()\n",
    "                result = do_call(entry)\n",
    "                chat_messages.append(result)\n",
    "            elif entry.type == 'message':\n",
    "                print(entry.content[0].text)\n",
    "                print()\n",
    "                has_messages = True\n",
    "\n",
    "        if has_messages:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704058a-5a92-4b23-a9d9-6917a9dce950",
   "metadata": {},
   "source": [
    "Same using widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "adc6338b-b742-431b-ac53-080d575fbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import markdown # pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b51483f5-2664-4c10-a3a4-56b47dab8465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_function_call(entry, result):\n",
    "        call_html = f\"\"\"\n",
    "            <details>\n",
    "            <summary>Function call: <tt>{entry.name}({shorten(entry.arguments)})</tt></summary>\n",
    "            <div>\n",
    "                <b>Call</b>\n",
    "                <pre>{entry}</pre>\n",
    "            </div>\n",
    "            <div>\n",
    "                <b>Output</b>\n",
    "                <pre>{result['output']}</pre>\n",
    "            </div>\n",
    "            \n",
    "            </details>\n",
    "        \"\"\"\n",
    "        display(HTML(call_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "029396f7-d86c-4320-af0c-ce67cf8e265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten(text, max_length=50):\n",
    "    if len(text) <= max_length:\n",
    "        return text\n",
    "\n",
    "    return text[:max_length - 3] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ace54679-8e40-41b9-8af4-1ceca2dbc409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_response(entry):\n",
    "        response_html = markdown.markdown(entry.content[0].text)\n",
    "        html = f\"\"\"\n",
    "            <div>\n",
    "                <div><b>Assistant:</b></div>\n",
    "                <div>{response_html}</div>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "        display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "491da276-42e8-4069-bc46-57fe2314c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " How do I do well in module 4?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"module 4 tips for success\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"module 4 tips for success\"}', call_id='call_bi1RnwKTndh3trO00TGZUCdp', name='search', type='function_call', id='fc_6878441c2df88192b962b9817ca154ad0dcd7fca0818ab9f', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\u201d\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\"` appropriately.\\nAdditionally, you can check for the version of \\u2018py4j\\u2019 of the spark you\\u2019re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 323\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Issue:\\ne\\u2026\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the \\u201c ModuleNotFoundError: No module named 'psycopg2' \\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 112\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\\\"Murray Hill\\\") & (new_final.b_zone==\\\"Midwood\\\")).show()\\nKrishna Anand\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 322\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \\\"TypeError: 'module' object is not callable\\\"\\nSolution:\\nconn_string = \\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\"\\nengine = create_engine(conn_string)\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Python - SQLALchemy - TypeError 'module' object is not callable\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 124\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Error raised during the jupyter notebook\\u2019s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module \\u201cpsycopg2\\u201d. Can be installed by Conda or pip.\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 125\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To do well in Module 4, here are some strategies you can adopt:</p>\n",
       "<ol>\n",
       "<li>\n",
       "<p><strong>Understand the Practical Applications</strong>: Focus on how the concepts you learn are applied in real-world scenarios. Try to relate them to your own projects or interests.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Engage with the Material</strong>: Participate actively in discussions and ask questions in forums. Engaging with peers can enhance your understanding.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Complete All Assignments</strong>: Ensure you complete all assignments and practice exercises. They are designed to reinforce learning and provide valuable hands-on experience.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Utilize Additional Resources</strong>: Look for tutorials, documentation, or forums related to the tools and techniques covered in this module. Sometimes a different perspective can clarify difficult concepts.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Practice Regularly</strong>: Set aside time to practice coding regularly. Hands-on experience will solidify your understanding and help you become more comfortable with the material.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Seek Help When Needed</strong>: If you encounter challenges, don’t hesitate to reach out for help—whether it’s from instructors or fellow students.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Would you like specific resources or study materials for Module 4?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what's the criteria to obtain course completion certificate?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"course completion certificate criteria\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"course completion certificate criteria\"}', call_id='call_iR3jjJ9oeZhpfnn63NXR1faY', name='search', type='function_call', id='fc_6878444b33908192a8ebaf6c6440e99b0dcd7fca0818ab9f', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"No, you can only get a certificate if you finish the course with a \\u201clive\\u201d cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Certificate - Can I follow the course in a self-paced mode and get a certificate?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 11\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Firstly, make sure that you add \\u201c!\\u201d before wget if you\\u2019re running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):\\nUsing the Python library wget you installed with pip, try python -m wget <url>\\nWrite the usual command and add --no-check-certificate at the end. So it should be:\\n!wget <website_url> --no-check-certificate\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"wget - ERROR: cannot verify <website> certificate  (MacOS)\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 49\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nThere's only one Data-Engineering Zoomcamp \\u201clive\\u201d cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you\\u2019re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any \\u201clive\\u201d cohort.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Course - how many Zoomcamps in a year?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 5\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  \\u201cOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon\\u2019t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Course - When will the course start?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 0\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors\\u2019 code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"How do I use Git / GitHub for this course?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 41\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To obtain a course completion certificate, you must meet the following criteria:</p>\n",
       "<ol>\n",
       "<li>\n",
       "<p><strong>Complete the Course in Live Mode</strong>: You need to participate in a live cohort; certificates are not awarded for self-paced modes.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Submission of Projects</strong>: You'll need to submit your projects, which will then require peer reviews from other participants.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Participation in Capstone Projects</strong>: Make sure to complete any capstone projects as required, which often have specific deadlines during the live cohort.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>This ensures that you engage fully with the material and collaborate with your peers, enriching the learning experience.</p>\n",
       "<p>Would you like more details on the live cohort schedule or how to participate in peer reviews?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "]\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    question = input()\n",
    "    if question.strip().lower() == 'stop':\n",
    "        print(\"Chat ended.\")\n",
    "        break\n",
    "    print()\n",
    "\n",
    "    message = {\"role\": \"user\", \"content\": question}\n",
    "    chat_messages.append(message)\n",
    "\n",
    "    while True:  # inner request loop\n",
    "        response = client.responses.create(\n",
    "            model='gpt-4o-mini',\n",
    "            input=chat_messages,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "        has_messages = False\n",
    "\n",
    "        for entry in response.output:\n",
    "            chat_messages.append(entry)\n",
    "\n",
    "            if entry.type == \"function_call\":\n",
    "                result = do_call(entry)\n",
    "                chat_messages.append(result)\n",
    "                display_function_call(entry, result)\n",
    "\n",
    "            elif entry.type == \"message\":\n",
    "                display_response(entry)\n",
    "                has_messages = True\n",
    "\n",
    "        if has_messages:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b17b9a-7e17-4d00-9d22-5c0b50102428",
   "metadata": {},
   "source": [
    "## Adding more tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2617748e-92a7-4425-8678-42f7d6416238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entry(question, answer):\n",
    "    doc = {\n",
    "        'question': question,\n",
    "        'text': answer,\n",
    "        'section': 'user added',\n",
    "        'course': 'data-engineering-zoomcamp'\n",
    "    }\n",
    "    index.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5a832e2b-6330-4e82-a223-5ae15b21c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_entry_description = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"add_entry\",\n",
    "    \"description\": \"Add an entry to the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The question to be added to the FAQ database\",\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The answer to the question\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"question\", \"answer\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a3aefc9e-ae3f-4a67-8984-cffaf0ab9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chat_assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "240044a0-58ea-4791-b050-5a0fd8142fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'Search query text to look up in the course FAQ.'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}},\n",
       " {'type': 'function',\n",
       "  'name': 'add_entry',\n",
       "  'description': 'Add an entry to the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'question': {'type': 'string',\n",
       "     'description': 'The question to be added to the FAQ database'},\n",
       "    'answer': {'type': 'string', 'description': 'The answer to the question'}},\n",
       "   'required': ['question', 'answer'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.add_tool(add_entry, add_entry_description)\n",
    "tools.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e7131fa5-0253-4038-a70d-e94d5e6a6fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'Search query text to look up in the course FAQ.'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}},\n",
       " {'type': 'function',\n",
       "  'name': 'add_entry',\n",
       "  'description': 'Add an entry to the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'question': {'type': 'string',\n",
       "     'description': 'The question to be added to the FAQ database'},\n",
       "    'answer': {'type': 'string', 'description': 'The answer to the question'}},\n",
       "   'required': ['question', 'answer'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.get_tools()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "04d92456-197c-497b-b291-c7a36e6822df",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_interface = chat_assistant.ChatInterface()\n",
    "\n",
    "chat = chat_assistant.ChatAssistant(\n",
    "    tools=tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b5bf443e-33b5-480d-ace8-d7091f9756ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: How do I score the highest points to top the leaderbord?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"highest points leaderboard\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"highest points leaderboard\"}', call_id='call_8CoiDBPbSlvjTh4BxSwUbsBU', name='search', type='function_call', id='fc_687845047b80819dbc36ecf99737b5b70b1b04ed331c0d28', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you\\u2019ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Homework and Leaderboard - what is the system for points in the course management platform?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 17\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"When you set up your account you are automatically assigned a random name such as \\u201cLucid Elbakyan\\u201d for example. If you want to see what your Display name is.\\nGo to the Homework submission link \\u2192  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on \\u2018Data Engineering Zoom Camp 2024\\u2019 > click on \\u2018Edit Course Profile\\u2019 - your display name is here, you can also change it should you wish:\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 18\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname, or your real name, if you prefer. Your entry on the Leaderboard is the one highlighted in teal(?) / light green (?).\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\nThe \\\"Display on Leaderboard\\\" option indicates whether you want your name to be listed on the course leaderboard.\\nQuestion: Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?\\nAnswer: Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.\",\n",
       "    \"section\": \"Workshop 1 - dlthub\",\n",
       "    \"question\": \"Edit Course Profile.\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 411\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"The video DE Zoomcamp 2.2.7 is missing  the actual deployment of Mage using Terraform to GCP. The steps for the deployment were not covered in the video.\\nI successfully deployed it and wanted to share some key points:\\nIn variables.tf, set the project_id default value to your GCP project ID.\\nEnable the Cloud Filestore API:\\nVisit the Google Cloud Console.to\\nNavigate to \\\"APIs & Services\\\" > \\\"Library.\\\"\\nSearch for \\\"Cloud Filestore API.\\\"\\nClick on the API and enable it.\\nTo perform the deployment:\\nterraform init\\nterraform apply\\nPlease note that during the terraform apply step, Terraform will prompt you to enter the PostgreSQL password. After that, it will ask for confirmation to proceed with the deployment. Review the changes, type 'yes' when prompted, and press Enter.\",\n",
       "    \"section\": \"Module 2: Workflow Orchestration\",\n",
       "    \"question\": \"GCP - 2.2.7d Deploying Mage to GCP\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 171\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\\nFor Windows, create a new User Variable \\u201cHADOOP_HOME\\u201d that points to your Hadoop directory. Then add \\u201c%HADOOP_HOME%\\\\bin\\u201d to the PATH variable.\\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Hadoop - Exception in thread \\\"main\\\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 354\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To score the highest points and top the leaderboard, you can focus on the following tasks:</p>\n",
       "<ol>\n",
       "<li>\n",
       "<p><strong>Homework Submissions</strong>: Complete all homework assignments thoroughly. Your points will be based on the number of questions answered correctly within each homework submission.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>FAQs</strong>: Contribute to the FAQ section. You earn one point for each question you submit that is included in the FAQ.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Learning in Public</strong>: Share your learning experiences through the \"Learning in Public\" initiative, where you can also earn one point for each shared link.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Make sure to stay engaged with the course and participate actively in all areas to maximize your points!</p>\n",
       "<p>Do you have a specific homework or topic you're currently working on?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: How do I fail the module?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"fail the module\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"fail the module\"}', call_id='call_NrCr39PTlOC6L0fb1jdldJZa', name='search', type='function_call', id='fc_6878452934f4819d95c05029d8fb7fc50b1b04ed331c0d28', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"Issue:\\ne\\u2026\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the \\u201c ModuleNotFoundError: No module named 'psycopg2' \\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 112\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
       "    \"section\": \"Module 4: analytics engineering with dbt\",\n",
       "    \"question\": \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 299\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\\\"Murray Hill\\\") & (new_final.b_zone==\\\"Midwood\\\")).show()\\nKrishna Anand\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 322\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\u201d\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\"` appropriately.\\nAdditionally, you can check for the version of \\u2018py4j\\u2019 of the spark you\\u2019re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 323\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Error raised during the jupyter notebook\\u2019s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module \\u201cpsycopg2\\u201d. Can be installed by Conda or pip.\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 125\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Failing a module is typically a result of not meeting the minimum requirements set by the instructors. Here are some common ways that could lead to failing:</p>\n",
       "<ol>\n",
       "<li>\n",
       "<p><strong>Insufficient Submissions</strong>: Not submitting your homework assignments or failing to meet deadlines consistently.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Poor Performance</strong>: Scoring significantly lower than the passing grade on assignments and assessments.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Lack of Participation</strong>: Not engaging in class discussions, group work, or the FAQ contributions that can earn you points.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Ignoring Feedback</strong>: Not taking into consideration the feedback provided by instructors on assignments.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>It's important to aim for improvement rather than aiming for failure! Is there a specific aspect of the module you're struggling with?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: how to install docker in gentoo?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"install docker in Gentoo\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"install docker in Gentoo\"}', call_id='call_xQfNmMPbQAURZiZJwAcg4kRC', name='search', type='function_call', id='fc_68784547fb54819da7da72b0412623670b1b04ed331c0d28', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"You may have this error:\\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\\n/simple/pandas/\\nPossible solution might be:\\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Docker - Cannot pip install on Docker container (Windows)\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 60\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"A:\\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\\n2) Use python ZipFile package, which is included in all modern python distributions\",\n",
       "    \"section\": \"Module 3: Data Warehousing\",\n",
       "    \"question\": \"Docker-compose takes infinitely long to install zip unzip packages for linux, which are required to unpack datasets\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 199\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"for windows if you having trouble install SDK try follow these steps on the link, if you getting this error:\\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\\nWARNING:\\nCannot find a quota project to add to ADC. You might receive a \\\"quota exceeded\\\" or \\\"API not enabled\\\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\\nFor me:\\nI reinstalled the sdk using unzip file \\u201cinstall.bat\\u201d,\\nafter successfully checking gcloud version,\\nrun gcloud init to set up project before\\nyou run gcloud auth application-default login\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md\\nGCP VM - I cannot get my Virtual Machine to start because GCP has no resources.\\nClick on your VM\\nCreate an image of your VM\\nOn the page of the image, tell GCP to create a new VM instance via the image\\nOn the settings page, change the location\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"GCP - Windows Google Cloud SDK install issue:gcp\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 134\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"terraformRun this command before starting your VM:\\nOn Intel CPU:\\nmodprobe -r kvm_intel\\nmodprobe kvm_intel nested=1\\nOn AMD CPU:\\nmodprobe -r kvm_amd\\nmodprobe kvm_amd nested=1\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Docker - Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization).\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 74\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"On few versions of Ubuntu, snap command can be used to install Docker.\\nsudo snap install docker\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Docker not installable on Ubuntu\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 77\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To install Docker on Gentoo, you can follow these general steps:</p>\n",
       "<ol>\n",
       "<li>\n",
       "<p><strong>Update Portage</strong>:\n",
       "   Make sure your package list is up-to-date by running:\n",
       "   <code>bash\n",
       "   sudo emerge --sync</code></p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Install Docker</strong>:\n",
       "   Use the following command to install Docker:\n",
       "   <code>bash\n",
       "   sudo emerge app-emulation/docker</code></p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Add User to Docker Group</strong>:\n",
       "   To run Docker commands without <code>sudo</code>, add your user to the Docker group:\n",
       "   <code>bash\n",
       "   sudo usermod -aG docker $USER</code></p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Start Docker Daemon</strong>:\n",
       "   You can start the Docker daemon using:\n",
       "   <code>bash\n",
       "   sudo systemctl start docker</code></p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Enable Docker to Start at Boot</strong>:\n",
       "   To enable Docker to start automatically when the system boots:\n",
       "   <code>bash\n",
       "   sudo systemctl enable docker</code></p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Verify Docker Installation</strong>:\n",
       "   You can verify that Docker is installed and running by checking the version:\n",
       "   <code>bash\n",
       "   docker --version</code></p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Make sure to replace any commands based on your specific version and configuration if needed!</p>\n",
       "<p>Do you have any other specific questions about using Docker or any issues you expect to encounter?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "chat.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "23c3e0db-b29e-4844-a0f9-0e3946cc2705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'Search query text to look up in the course FAQ.'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}},\n",
       " {'type': 'function',\n",
       "  'name': 'add_entry',\n",
       "  'description': 'Add an entry to the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'question': {'type': 'string',\n",
       "     'description': 'The question to be added to the FAQ database'},\n",
       "    'answer': {'type': 'string', 'description': 'The answer to the question'}},\n",
       "   'required': ['question', 'answer'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.add_tool(add_entry, add_entry_description)\n",
    "tools.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "284ca4af-c997-4e9b-ae29-4d37cff4dd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'Search query text to look up in the course FAQ.'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}},\n",
       " {'type': 'function',\n",
       "  'name': 'add_entry',\n",
       "  'description': 'Add an entry to the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'question': {'type': 'string',\n",
       "     'description': 'The question to be added to the FAQ database'},\n",
       "    'answer': {'type': 'string', 'description': 'The answer to the question'}},\n",
       "   'required': ['question', 'answer'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8c01727c-0f04-4788-bb6c-9954f05b8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = chat_assistant.ChatAssistant(\n",
    "    tools=tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "24c594de-990e-4692-b0f6-23af522b0b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: how can i install kubernetes on ubuntu?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"install kubernetes on ubuntu\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"install kubernetes on ubuntu\"}', call_id='call_2Oxa4prPCsfW6gFmTyLFJTW1', name='search', type='function_call', id='fc_68784614aa8081918d81e24b72ec80150ef27546bca9cdd7', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"On few versions of Ubuntu, snap command can be used to install Docker.\\nsudo snap install docker\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Docker not installable on Ubuntu\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 77\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"A:\\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\\n2) Use python ZipFile package, which is included in all modern python distributions\",\n",
       "    \"section\": \"Module 3: Data Warehousing\",\n",
       "    \"question\": \"Docker-compose takes infinitely long to install zip unzip packages for linux, which are required to unpack datasets\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 199\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"For this workshop, and if you are following the view from Noel (2024) this requires you to install postgres to use it on your terminal. Found this steps (commands) to get it done [source]:\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\nsudo sh -c 'echo \\\"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\\\" >> /etc/apt/sources.list.d/pgdg.list'\\nsudo apt update\\napt install postgresql postgresql-contrib\\n(comment): now let\\u2019s check the service for postgresql\\nservice postgresql status\\n(comment) If down: use the next command\\nservice postgresql start\\n(comment) And your are done\",\n",
       "    \"section\": \"Workshop 2 - RisingWave\",\n",
       "    \"question\": \"How to install postgress on Linux like OS\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 429\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"If you\\u2019re using an Anaconda installation:\\nCd home/\\nConda install gcc\\nSource back to your RisingWave Venv - source .venv/bin/activate\\nPip install psycopg2-binary\\nPip install -r requirements.txt\\nFor some reason this worked - the Conda base doesn\\u2019t have the GCC installed - (GNU Compiler Collection) a compiler system that supports various programming languages. Without this the it fails to install pyproject.toml-based projects\\n\\u201cIt's possible that in your specific environment, the gcc installation was required at the system level rather than within the virtual environment. This can happen if the build process for psycopg2 tries to access system-level dependencies during installation.\\nInstalling gcc in your main Python installation (Conda) would make it available system-wide, allowing any Python environment to access it when necessary for building packages.\\u201d\\ngcc stands for GNU Compiler Collection. It is a compiler system developed by the GNU Project that supports various programming languages, including C, C++, Objective-C, and Fortran.\\nGCC is widely used for compiling source code written in these languages into executable programs or libraries. It's a key tool in the software development process, particularly in the compilation stage where source code is translated into machine code that can be executed by a computer's processor.\\nIn addition to compiling source code, GCC also provides various optimization options, debugging support, and extensive documentation, making it a powerful and versatile tool for developers across different platforms and architectures.\\n\\u2014-----------------------------------------------------------------------------------\",\n",
       "    \"section\": \"Workshop 2 - RisingWave\",\n",
       "    \"question\": \"Psycopg2 - `Could not build wheels for psycopg2, which is required to install pyproject.toml-based projects`\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 424\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"psql is a command line tool that is installed alongside PostgreSQL DB, but since we've always been running PostgreSQL in a container, you've only got `pgcli`, which lacks the feature to run a sql script into the DB. Besides, having a command line for each database flavor you'll have to deal with as a Data Professional is far from ideal.\\nSo, instead, you can use usql. Check the docs for details on how to install for your OS. On macOS, it supports `homebrew`, and on Windows, it supports scoop.\\nSo, to run the taxi_trips.sql script with usql:\",\n",
       "    \"section\": \"Workshop 2 - RisingWave\",\n",
       "    \"question\": \"psql - command not found: psql (alternative install)\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 417\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To install Kubernetes on Ubuntu, you can follow these general steps:</p>\n",
       "<ol>\n",
       "<li>\n",
       "<p><strong>Set Up the Environment</strong>:\n",
       "   Make sure you have Docker installed. You can use the <code>snap</code> command if it's supported:\n",
       "   <code>bash\n",
       "   sudo snap install docker</code></p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Add Kubernetes APT Repository</strong>:\n",
       "   Import the Google Cloud public signing key:\n",
       "   <code>bash\n",
       "   curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -</code>\n",
       "   Then add the Kubernetes APT repository:\n",
       "   <code>bash\n",
       "   sudo sh -c 'echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" &gt; /etc/apt/sources.list.d/kubernetes.list'</code></p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Install Kubernetes</strong>:\n",
       "   Update your package list and install Kubernetes:\n",
       "   <code>bash\n",
       "   sudo apt-get update\n",
       "   sudo apt-get install -y kubelet kubeadm kubectl</code></p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Hold the Version</strong>:\n",
       "   To prevent Kubernetes from being updated automatically:\n",
       "   <code>bash\n",
       "   sudo apt-mark hold kubelet kubeadm kubectl</code></p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Initialize Kubernetes Cluster</strong>:\n",
       "   You can initialize your Kubernetes cluster using:\n",
       "   <code>bash\n",
       "   sudo kubeadm init</code>\n",
       "   Follow the on-screen instructions for the necessary commands to set up your <code>kubectl</code>.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Set Up the <code>kubectl</code> Configuration</strong>:\n",
       "   To start using your cluster, set up your <code>kubectl</code> configuration:\n",
       "   <code>bash\n",
       "   mkdir -p $HOME/.kube\n",
       "   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n",
       "   sudo chown $(id -u):$(id -g) $HOME/.kube/config</code></p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Install a Pod Network</strong> (Optional):\n",
       "   You might want to install a pod network (like Calico or Flannel) to allow communication between pods.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>These steps should help you get started with Kubernetes on Ubuntu. Make sure to consult the official Kubernetes documentation for the latest configurations or installation methods.</p>\n",
       "<p>Would you like to know more about using Kubernetes after installation, or do you have other questions?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: Add this info to the index\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>add_entry({\"question\":\"How can I install Kubernetes on Ub...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"question\":\"How can I install Kubernetes on Ubuntu?\",\"answer\":\"To install Kubernetes on Ubuntu, follow these steps:\\\\n\\\\n1. **Set Up the Environment**: Ensure Docker is installed:\\\\n   ```bash\\\\n   sudo snap install docker\\\\n   ```\\\\n\\\\n2. **Add Kubernetes APT Repository**: Import the Google Cloud public signing key:\\\\n   ```bash\\\\n   curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\\\\n   ```\\\\n   Add the Kubernetes APT repository:\\\\n   ```bash\\\\n   sudo sh -c \\'echo \\\\\"deb http://apt.kubernetes.io/ kubernetes-xenial main\\\\\" > /etc/apt/sources.list.d/kubernetes.list\\'\\\\n   ```\\\\n\\\\n3. **Install Kubernetes**: Update package list and install:\\\\n   ```bash\\\\n   sudo apt-get update\\\\n   sudo apt-get install -y kubelet kubeadm kubectl\\\\n   ```\\\\n\\\\n4. **Hold the Version**: Prevent automatic updates:\\\\n   ```bash\\\\n   sudo apt-mark hold kubelet kubeadm kubectl\\\\n   ```\\\\n\\\\n5. **Initialize Kubernetes Cluster**: Initialize the cluster:\\\\n   ```bash\\\\n   sudo kubeadm init\\\\n   ```\\\\n   Follow on-screen instructions.\\\\n\\\\n6. **Set Up the `kubectl` Configuration**: Set up `kubectl`:\\\\n   ```bash\\\\n   mkdir -p $HOME/.kube\\\\n   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\\\\n   sudo chown $(id -u):$(id -g) $HOME/.kube/config\\\\n   ```\\\\n\\\\n7. **Install a Pod Network** (Optional): Install a pod network (like Calico or Flannel) to allow communication between pods.\"}', call_id='call_uXI19APhMkkAMpp0joDnO0Ed', name='add_entry', type='function_call', id='fc_6878462e8d1881918ec727bd0106920a0ef27546bca9cdd7', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>null</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>I've added the information on installing Kubernetes on Ubuntu to the FAQ database. </p>\n",
       "<p>If you have any more questions or if there's anything else you'd like to learn about Kubernetes, feel free to ask!</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "chat.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b66040ec-a146-47d7-aaec-53322100987a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How can I install Kubernetes on Ubuntu?',\n",
       " 'text': 'To install Kubernetes on Ubuntu, follow these steps:\\n\\n1. **Set Up the Environment**: Ensure Docker is installed:\\n   ```bash\\n   sudo snap install docker\\n   ```\\n\\n2. **Add Kubernetes APT Repository**: Import the Google Cloud public signing key:\\n   ```bash\\n   curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\\n   ```\\n   Add the Kubernetes APT repository:\\n   ```bash\\n   sudo sh -c \\'echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" > /etc/apt/sources.list.d/kubernetes.list\\'\\n   ```\\n\\n3. **Install Kubernetes**: Update package list and install:\\n   ```bash\\n   sudo apt-get update\\n   sudo apt-get install -y kubelet kubeadm kubectl\\n   ```\\n\\n4. **Hold the Version**: Prevent automatic updates:\\n   ```bash\\n   sudo apt-mark hold kubelet kubeadm kubectl\\n   ```\\n\\n5. **Initialize Kubernetes Cluster**: Initialize the cluster:\\n   ```bash\\n   sudo kubeadm init\\n   ```\\n   Follow on-screen instructions.\\n\\n6. **Set Up the `kubectl` Configuration**: Set up `kubectl`:\\n   ```bash\\n   mkdir -p $HOME/.kube\\n   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\\n   sudo chown $(id -u):$(id -g) $HOME/.kube/config\\n   ```\\n\\n7. **Install a Pod Network** (Optional): Install a pod network (like Calico or Flannel) to allow communication between pods.',\n",
       " 'section': 'user added',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.docs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bc58924f-1ce4-4bfa-812e-2362c6958c5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic-ai in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (0.4.2)\n",
      "Requirement already satisfied: pydantic-ai-slim==0.4.2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.4.2)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.2.2)\n",
      "Requirement already satisfied: griffe>=1.3.2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.7.3)\n",
      "Requirement already satisfied: httpx>=0.27 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.28.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.28.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.35.0)\n",
      "Requirement already satisfied: pydantic-graph==0.4.2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.4.2)\n",
      "Requirement already satisfied: pydantic>=2.10 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.11.7)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.4.1)\n",
      "Requirement already satisfied: anthropic>=0.52.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.57.1)\n",
      "Requirement already satisfied: boto3>=1.37.24 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.39.6)\n",
      "Requirement already satisfied: argcomplete>=3.5.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (3.6.2)\n",
      "Requirement already satisfied: prompt-toolkit>=3 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (3.0.43)\n",
      "Requirement already satisfied: rich>=13 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (14.0.0)\n",
      "Requirement already satisfied: cohere>=5.13.11 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (5.16.1)\n",
      "Requirement already satisfied: pydantic-evals==0.4.2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.4.2)\n",
      "Requirement already satisfied: google-genai>=1.24.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.25.0)\n",
      "Requirement already satisfied: groq>=0.19.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.30.0)\n",
      "Requirement already satisfied: mcp>=1.9.4 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.11.0)\n",
      "Requirement already satisfied: mistralai>=1.2.5 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.9.2)\n",
      "Requirement already satisfied: openai>=1.76.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.93.3)\n",
      "Requirement already satisfied: google-auth>=2.36.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.40.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.32.4)\n",
      "Requirement already satisfied: anyio>=0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-evals==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (4.9.0)\n",
      "Requirement already satisfied: logfire-api>=1.2.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-evals==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (3.24.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-evals==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (6.0.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from anyio>=0->pydantic-evals==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (3.7)\n",
      "Requirement already satisfied: certifi in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from httpx>=0.27->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from httpx>=0.27->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic>=2.10->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic>=2.10->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.33.2)\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.6 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from boto3>=1.37.24->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.39.6)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from boto3>=1.37.24->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from boto3>=1.37.24->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from botocore<1.40.0,>=1.39.6->boto3>=1.37.24->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from botocore<1.40.0,>=1.39.6->boto3>=1.37.24->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.40.0,>=1.39.6->boto3>=1.37.24->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.16.0)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.11.1)\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.4.0)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.21.1)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.32.4.20250611)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.0.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.33.0)\n",
      "Requirement already satisfied: filelock in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (23.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (4.66.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth>=2.36.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.4.8)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from google-genai>=1.24.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (8.5.0)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from google-genai>=1.24.0->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (15.0.1)\n",
      "Requirement already satisfied: colorama>=0.4 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from griffe>=1.3.2->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.4.6)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (4.24.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.0.20)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.4.1)\n",
      "Requirement already satisfied: starlette>=0.27 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.45.3)\n",
      "Requirement already satisfied: uvicorn>=0.23.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.35.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.10.6)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.28.0->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim==0.4.2->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (3.17.0)\n",
      "Requirement already satisfied: wcwidth in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from prompt-toolkit>=3->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.2.5)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from pydantic-settings>=2.5.2->mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (1.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from rich>=13->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from rich>=13->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (0.1.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/sparsh-raj/anaconda3/lib/python3.12/site-packages (from uvicorn>=0.23.1->mcp>=1.9.4->pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.4.2->pydantic-ai) (8.1.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydantic-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cfce3d4c-0277-429d-9503-11e2e0957434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent, RunContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3e4cf830-5e34-453f-a1f5-90f60d33ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_agent = Agent(  \n",
    "    'openai:gpt-4o-mini',\n",
    "    system_prompt=developer_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a87a8389-3e58-4cd7-8e3e-e261d93db68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "235631e7-2f63-41bc-8a75-443dc33ce471",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chat_agent.tool\n",
    "def search_tool(ctx: RunContext, query: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Search the FAQ for relevant entries matching the query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : str\n",
    "        The search query string provided by the user.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of search results (up to 5), each containing relevance information \n",
    "        and associated output IDs.\n",
    "    \"\"\"\n",
    "    print(f\"search('{query}')\")\n",
    "    return search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3d6f79e4-26df-46cc-9fb1-a93785e80ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chat_agent.tool\n",
    "def add_entry_tool(ctx: RunContext, question: str, answer: str) -> None:\n",
    "    \"\"\"\n",
    "    Add a new question-answer entry to FAQ.\n",
    "\n",
    "    This function creates a document with the given question and answer, \n",
    "    tagging it as user-added content.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "        The question text to be added to the index.\n",
    "\n",
    "    answer : str\n",
    "        The answer or explanation corresponding to the question.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    return add_entry(question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8c9e5fdb-c087-4566-824b-6d9777f20e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search('Can I join the course now?')\n",
      "Yes, you can still join the course even if it has already started. You will be eligible to submit homework assignments, but keep in mind that there are deadlines for turning in final projects, so it's best not to leave anything to the last minute.\n",
      "\n",
      "Would you like more details about the specific deadlines or requirements for the course?\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"I just discovered the course. Can I join now?\"\n",
    "agent_run = await chat_agent.run(user_prompt)\n",
    "print(agent_run.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18dfe6d-bfe9-4b52-893c-7fc3eefae240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
