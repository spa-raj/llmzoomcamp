import pandas as pd


import requests

docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'
docs_response = requests.get(docs_url)
documents_raw = docs_response.json()

documents = []

for course in documents_raw:
    course_name = course['course']

    for doc in course['documents']:
        doc['course'] = course_name
        documents.append(doc)





df = pd.DataFrame(documents, columns=['course', 'section', 'question', 'text'])
df.head()


df.course.unique()


df[df.course == 'machine-learning-zoomcamp'].head(-5)


docs_example = [
    "Course starts on 15th Jan 2024",
    "Prerequisites listed on GitHub",
    "Submit homeworks after start date",
    "Registration not required for participation",
    "Setup Google Cloud and Python before course"
]


from sklearn.feature_extraction.text import CountVectorizer


cv = CountVectorizer(stop_words='english')
X = cv.fit_transform(docs_example)

names = cv.get_feature_names_out()

df_docs = pd.DataFrame(X.todense(), columns=names).T
df_docs


from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(stop_words='english')
X = cv.fit_transform(docs_example)

names = cv.get_feature_names_out()

df_docs = pd.DataFrame(X.toarray(), columns=names).T
df_docs


from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(stop_words='english' ,min_df=5)
X = cv.fit_transform(df.text)

names = cv.get_feature_names_out()

df_docs = pd.DataFrame(X.todense(), columns=names)
df_docs


from sklearn.feature_extraction.text import TfidfVectorizer

cv = TfidfVectorizer(stop_words='english' ,min_df=5)
X = cv.fit_transform(df.text)

names = cv.get_feature_names_out()

df_docs = pd.DataFrame(X.todense(), columns=names)
df_docs





query = "Do I need to know python to sign up for the January course?"

q = cv.transform([query])
q.toarray()


query_dict = dict(zip(names, q.toarray()[0]))
query_dict


doc_dict = dict(zip(names, X.toarray()[1]))
doc_dict


df_qd = pd.DataFrame([query_dict, doc_dict], index=['query', 'doc']).T

(df_qd['query'] * df_qd['doc']).sum()


X.dot(q.T).toarray()


from sklearn.metrics.pairwise import cosine_similarity
cosine_similarity(X, q)


fields = ['section', 'question', 'text']
transformers = {}
matrices = {}

for field in fields:
    cv = TfidfVectorizer(stop_words='english', min_df=3)
    X = cv.fit_transform(df[field])

    transformers[field] = cv
    matrices[field] = X

transformers['text'].get_feature_names_out()
matrices['text']


matrices, transformers


query = "I just signed up. Is it too late to join the course?"

q = transformers['text'].transform([query])
score = cosine_similarity(matrices['text'], q).flatten()


mask = (df.course == 'data-engineering-zoomcamp').values
score = score * mask


import numpy as np

idx = np.argsort(-score)[:10]


df.iloc[idx].text


score = np.zeros(len(df), dtype=float)
query = "I just signed up. Is it too late to join the course?"

for field in fields:
    q = transformers[field].transform([query])
    score += cosine_similarity(matrices[field], q).flatten()


idx = np.argsort(-score)[:10]
df.iloc[idx]


filters = {
    'course': 'data-engineering-zoomcamp'
}


for field, value in filters.items():
    mask = (df[field] == value).astype(int).values
    score = score * mask


idx = np.argsort(-score)[:10]
df.iloc[idx]


score = np.zeros(len(df), dtype=float)
query = "I just signed up. Is it too late to join the course?"
boost = { 'question': 3.0,
             # 'section': 0.5,
    # 'text': 2.0
        }

for field in fields:
    q = transformers[field].transform([query])
    f_score =  cosine_similarity(matrices[field], q).flatten()
    score += f_score * boost.get(field, 1.0)


ilters = {
    'course': 'data-engineering-zoomcamp'
}
for field, value in filters.items():
    mask = (df[field] == value).astype(int).values
    score = score * mask
idx = np.argsort(-score)[:5]
df.iloc[idx]


class TextSearch:

    def __init__(self, text_fields):
        self.text_fields = text_fields
        self.matrices = {}
        self.vectorizers = {}

    def fit(self, records, vectorizer_params={}):
        self.df = pd.DataFrame(records)

        for f in self.text_fields:
            cv = TfidfVectorizer(**vectorizer_params)
            X = cv.fit_transform(self.df[f])
            self.matrices[f] = X
            self.vectorizers[f] = cv

    def search(self, query, n_results=10, boost={}, filters={}):
        score = np.zeros(len(self.df))

        for f in self.text_fields:
            b = boost.get(f, 1.0)
            q = self.vectorizers[f].transform([query])
            s = cosine_similarity(self.matrices[f], q).flatten()
            score = score + b * s

        for field, value in filters.items():
            mask = (self.df[field] == value).values
            score = score * mask

        idx = np.argsort(-score)[:n_results]
        results = self.df.iloc[idx]
        return results.to_dict(orient='records')


index = TextSearch(
    text_fields=['section', 'question', 'text']
)
index.fit(documents)

index.search(
    query='I just signed up. Is it too late to join the course?',
    n_results=5,
    boost={'question': 3.0},
    filters={'course': 'data-engineering-zoomcamp'}
)


from sklearn.decomposition import TruncatedSVD

X = matrices['text']
cv = transformers['text']

svd = TruncatedSVD(n_components=16)
X_emb = svd.fit_transform(X)

X_emb[0]


query = 'I just signed up. Is it too late to join the course?'

Q = cv.transform([query])
Q_emb = svd.transform(Q)
Q_emb[0]


np.dot(X_emb[0], Q_emb[0])


score = cosine_similarity(X_emb, Q_emb).flatten()
idx = np.argsort(-score)[:10]
df.loc[idx]





from sklearn.decomposition import NMF
nmf = NMF(n_components=16)
X_emb = nmf.fit_transform(X)
X_emb[0]


Q = cv.transform([query])
Q_emb = nmf.transform(Q)
Q_emb[0]


score = cosine_similarity(X_emb, Q_emb).flatten()
idx = np.argsort(-score)[:10]
df.loc[idx]





pip install transformers tqdm torch


import torch
from transformers import BertModel, BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")
model.eval()  # Set the model to evaluation mode if not training


texts = [
    "Yes, we will keep all the materials after the course finishes.",
    "You can follow the course at your own pace after it finishes"
]
encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')


with torch.no_grad():  # Disable gradient calculation for inference
    outputs = model(**encoded_input)
    hidden_states = outputs.last_hidden_state


sentence_embeddings = hidden_states.mean(dim=1)
sentence_embeddings.shape


X_emb = sentence_embeddings.numpy()


def make_batches(seq, n):
    result = []
    for i in range(0, len(seq), n):
        batch = seq[i:i+n]
        result.append(batch)
    return result


from tqdm.auto import tqdm


def compute_embeddings(texts, batch_size=8):
    text_batches = make_batches(texts, 8)

    all_embeddings = []

    for batch in tqdm(text_batches):
        encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')

        with torch.no_grad():
            outputs = model(**encoded_input)
            hidden_states = outputs.last_hidden_state

            batch_embeddings = hidden_states.mean(dim=1)
            batch_embeddings_np = batch_embeddings.cpu().numpy()
            all_embeddings.append(batch_embeddings_np)

    final_embeddings = np.vstack(all_embeddings)
    return final_embeddings


embeddings = {}


for f in fields:
    print(f'computing embeddings for {f}...')
    embeddings[f] = compute_embeddings(df[f].tolist())
